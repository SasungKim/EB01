{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#text preprocessing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "#from textblob import TextBlob\n",
    "from textblob import TextBlob, Word, Blobber\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob.taggers import NLTKTagger\n",
    "\n",
    "# enable multiple outputs per cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(fileName):\n",
    "    df = pd.read_csv(f'./datasets/data_cleaned_{fileName}.csv'\n",
    "                     , sep=',', encoding='utf-8')\n",
    "\n",
    "    ref = pickle.load(open('./news_data/reference.csv', 'rb'))\n",
    "\n",
    "    ref_df = pd.DataFrame()\n",
    "\n",
    "    statement_list = []\n",
    "    statement_sub = []\n",
    "\n",
    "    #print(ref['coronavirus'].head(5))\n",
    "    #print(ref['coronavirus'].iloc[:])\n",
    "\n",
    "    topics = ref.keys()\n",
    "\n",
    "    for sub in topics:\n",
    "        a = ref[sub].values.tolist()\n",
    "        for statements in a:\n",
    "            for s in statements:\n",
    "                statement_list.append(s)\n",
    "        for rows in range(len(ref[sub])):\n",
    "            statement_sub.append(sub)\n",
    "\n",
    "    #print(type(statement_list[1]))\n",
    "    #print(len(statement_sub))\n",
    "\n",
    "    ref_df['statement'] = statement_list\n",
    "    ref_df['subject'] = statement_sub\n",
    "    ref_df = ref_df.dropna(how = 'any')\n",
    "    a = np.ones(len(ref_df))\n",
    "    ref_df['label'] = a\n",
    "    #print(type(ref_df))\n",
    "    return df, ref_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defining functions for feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### punctuation and symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_symbol(statement, symbol):\n",
    "    return len(statement) - len(statement.replace(symbol, ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: String('str')\n",
    "# Description: Count the number of characters in input\n",
    "# Return: Character count - int ('count')\n",
    "\n",
    "def count_char(str):\n",
    "    no_space = str.replace(\" \", \"\")\n",
    "    count = len(no_space)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: String('str')\n",
    "# Description: Count the number of words in input\n",
    "# Return: Word count - int ('count')\n",
    "\n",
    "def count_word(str):\n",
    "    count = len(str.split())\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: String ('str')\n",
    "# Description: Count the number of sentences by counting number of period(.)\n",
    "# Return: Sentence count - int ('sentence')\n",
    "\n",
    "def count_sent(str):\n",
    "    sentence = len(str.split('.'))\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: String ('str')\n",
    "# Description: Count the number of characters in each word in input and average the number of characters per word\n",
    "# Return: Average number of characters: float ('avg')\n",
    "\n",
    "def count_char_per_word(str):\n",
    "    avg = 0\n",
    "    word = []\n",
    "    word.append(str.split())\n",
    "    char_per_word = list()\n",
    "    for elements in word:\n",
    "        for char in elements:\n",
    "            c_in_w_count = len(char)\n",
    "            char_per_word.append(c_in_w_count)\n",
    "    try:\n",
    "        # char_per_word_list.append()\n",
    "        avg = sum(char_per_word) / len(char_per_word)\n",
    "        char_per_word.clear()\n",
    "    except:\n",
    "        print(str)\n",
    "        \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: String ('str')\n",
    "# Description: Count the words that introduced only once in input\n",
    "# Return: Count of unique words - int ('unique_count')\n",
    "\n",
    "def count_unique(str):\n",
    "    words = str.split(' ')\n",
    "    c = Counter(words)\n",
    "    unique = [w for w in words if c[w] == 1]\n",
    "    unique_counter = len(unique)\n",
    "    return unique_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: String ('str')\n",
    "# Description: Count the number of uppercase letters\n",
    "# Return: Count of uppercase letters - int ('uppercase_count')\n",
    "\n",
    "def count_uppercase(str):\n",
    "    uppercase_count = sum(1 for c in str if c.isupper())\n",
    "    return uppercase_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: String ('str')\n",
    "# Description: Count the number of month name mentioned\n",
    "# Return: Count of month name - int ('month_count')\n",
    "\n",
    "def count_month(str):\n",
    "    month_count = 0\n",
    "    month_list = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\"\n",
    "                  , \"July\", \"August\", \"September\", \"October\", \"November\"\n",
    "                  , \"December\"];\n",
    "    words = str.split()\n",
    "    for word in words:\n",
    "        if word in month_list:\n",
    "            month_count+=1\n",
    "        \n",
    "    return month_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment score calculation\n",
    "# sentiment score, polarity, subjectivity and intensity can be calculated\n",
    "def sentiment_score(str):\n",
    "    sentiment = TextBlob(str)\n",
    "    return sentiment.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation\n",
    "import string\n",
    "def remove_symbol(str):\n",
    "    no_symbol = re.sub(r'[^\\w\\s]','',str)\n",
    "    return no_symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_feature(df):\n",
    "    # counting symbols\n",
    "    symbol_list = list('-?!%;:\"($,.')\n",
    "    for symbol in symbol_list:\n",
    "        df['num_'+symbol] = df.statement.apply(lambda x: count_symbol(x, symbol))\n",
    "        # remove symbols to prepare to text processing\n",
    "        df_raw = df.copy() # make a copy before transforming just in case\n",
    "        df.statement = df.statement.apply(lambda x: remove_symbol(x))\n",
    "        \n",
    "        #ref_df['num_' + symbol] = df.statement.apply(lambda x: count_symbol(x, symbol))\n",
    "        #ref_df.statement = ref_df.statement.apply(lambda x: remove_symbol(x))\n",
    "\n",
    "    # text processing that takes in statements\n",
    "    feature_func = [count_char, count_word, count_sent, count_char_per_word\n",
    "                , count_unique, count_uppercase, count_month\n",
    "                , sentiment_score]\n",
    "\n",
    "    for func in feature_func:\n",
    "        df[func.__name__] = df.statement.apply(lambda x: func(x))\n",
    "        #ref_df[func.__name__] = ref_df.statement.apply(lambda x: func(x))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_common(str, list):\n",
    "    count = 0;\n",
    "    words = str.split()\n",
    "    for word in words:\n",
    "        if word in list:\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagging_univ(str):\n",
    "    text = nltk.word_tokenize(str)\n",
    "    tagged = nltk.pos_tag(text, tagset = 'universal')\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagging_nuniv(str):\n",
    "    text = nltk.word_tokenize(str)\n",
    "    tagged = nltk.pos_tag(text)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(df, fileName):\n",
    "    liwc_headers = ['function','pronoun','ppron','i','we','you','shehe'\n",
    "              ,'they','ipron','article','prep','auxverb','adverb'\n",
    "              ,'conj','negate','verb','adj','compare','interrog'\n",
    "              ,'number','quant','affect','posemo','negemo','anx','anger'\n",
    "              ,'sad','social','family','friend','female','male','cogproc'\n",
    "              ,'insight','cause','discrep','tentat','certain','differ'\n",
    "              ,'percept','see','hear','feel','bio','body','health','sexual'\n",
    "              ,'ingest','drives','affiliation','achieve','power','reward'\n",
    "              ,'risk','focuspast','focuspresent','focusfuture','relativ'\n",
    "              ,'motion','space','time','work','leisure','home','money'\n",
    "              ,'relig','death','informal','swear','netspeak','assent'\n",
    "              ,'nonflu','filler']\n",
    "\n",
    "    liwc_dict = pd.read_csv('./datasets/LIWC_dict/LIWC_dictionary.csv'\n",
    "                      , delimiter = ',', names = liwc_headers\n",
    "                      , encoding = 'utf-8-sig')\n",
    "    liwc_dict = liwc_dict.dropna()\n",
    "\n",
    "\n",
    "    for header in liwc_headers:\n",
    "        df['count_'+header] = df.statement.apply(\n",
    "            lambda x: check_common(x, header))\n",
    "\n",
    "        #ref_df['count_'+header] = ref_df.statement.apply(\n",
    "            #lambda x: check_common(x, header))\n",
    "\n",
    "    df.to_csv(f'./datasets/feature_extracted_{fileName}.csv', sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "    #ref_df.to_csv(f'./datasets/ref_feature_extracted_{fileName}.csv', sep=',', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "#cv = CountVectorizer(stop_words='english')\n",
    "#text_cv = cv.fit_transform(df.statement.values.astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf, better than bag of words\n",
    "#tfidf = TfidfVectorizer(norm=None)\n",
    "#text_tfidf = tfidf.fit_transform(df.statement).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, ref_df_train = setup('train')\n",
    "test_df, ref_df_test = setup('test')\n",
    "valid_df, ref_df_valid = setup('valid')\n",
    "\n",
    "train_df = text_feature(train_df)\n",
    "test_df = text_feature(test_df)\n",
    "valid_df = text_feature(valid_df)\n",
    "\n",
    "feature_extraction(train_df, 'train')\n",
    "feature_extraction(test_df, 'test')\n",
    "feature_extraction(valid_df, 'valid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multisource features - not done yet, no need to run after this part for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract each feature values for each data in the same row (one-to-one). List contains\n",
    "# the extracted feature values related to texts. Ex. feature_list[0] = all feature values of first statement\n",
    "# In case of there is null in the reference data, use two next reference data\n",
    "\n",
    "def one_to_one_dif(text, ref, subject_list):\n",
    "    feature_list = []\n",
    "    feature = 0\n",
    "    for i in range (len(text)):\n",
    "        topics = subject_list.iloc[i]\n",
    "        topics = topics.split(',')\n",
    "\n",
    "        try:\n",
    "            feature = (float(text[i]) - float(ref[topic[0]][i]))\n",
    "        except:\n",
    "            try:\n",
    "                feature = (float(text[i]) - float(ref[i+2]))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        feature_list.append(round(feature, 2))\n",
    "    return feature_list\n",
    "    \n",
    "# Subtract each feature values for each data in the same row and repeat it with different references.(one-to-many)\n",
    "# Then, average the difference. List contains the extracted feature values related to texts.\n",
    "# Ex. final_list[0] = all feature values of first statement\n",
    "# In case of there is null in the reference data, use two next reference data\n",
    "\n",
    "def one_to_many_dif(text, ref, subject_list):\n",
    "    avg_list = []\n",
    "    final_list = []\n",
    "    reference = pd.DataFrame()\n",
    "    for i in range(len(text)):\n",
    "        topics = subject_list.iloc[i]\n",
    "        topics = topics.split(',')\n",
    "        for topic in topics:\n",
    "            reference = pd.concat([reference, ref[topic]], ignoreIndex = True)\n",
    "        for j in range(len(ref)):\n",
    "            try:\n",
    "                feature_val = float(text[i]) - float(ref[j])\n",
    "            except:\n",
    "                try:\n",
    "                    feature_val = float(text[i]) - float(ref[j+2])\n",
    "                except:\n",
    "                    pass\n",
    "            avg_list.append(feature_val)\n",
    "        avg = round(mean(avg_list), 2)\n",
    "        final_list.append(avg)\n",
    "    return final_list\n",
    "    \n",
    "    \n",
    "# Input: news1 (String), news2 (String), outFile (String), startFColumn (integer)\n",
    "# Description: Read the data from news1 (reliable) and news2 (unreliable), and put the data in two lists. Then, subtract the news1 values in each column from news2 values.\n",
    "#              The subtraction starts from startFColumn until the end of the .csv file. Next, the results will be saved in the file with given name or path from user (outFile)\n",
    "#\n",
    "# Return: New DataFrame ('MS_features')\n",
    "\n",
    "def multi_source_FE(text, ref):\n",
    "\n",
    "    # Saving original headers in header\n",
    "    header = list(text)\n",
    "    \n",
    "    subjects = set(ref.subject)\n",
    "    topic = dict()\n",
    "    \n",
    "    for subject in subjects:\n",
    "        topic[subject] = ref.loc[ref['subject'] == subject]\n",
    "        \n",
    "    # Only feature values\n",
    "    cols = [col for col in text.columns if col not in ['statement', 'label', 'subject']]\n",
    "    \n",
    "    state_text = text['statement']\n",
    "    state_ref = ref['statement']\n",
    "    label_text = text['label']\n",
    "    subject_text = text['subject']\n",
    "    data_text = text[cols]\n",
    "    data_ref = ref[cols]\n",
    "\n",
    "    feature_num = len(data_text.keys())\n",
    "\n",
    "\n",
    "    # Create local lists, dataframe that are used later in this definition.\n",
    "    df = pd.DataFrame()\n",
    "    sub = pd.DataFrame()\n",
    "    avg_sub = pd.DataFrame()\n",
    "    text = []\n",
    "    ref = []\n",
    "    total_list = []\n",
    "    new_total_list = []\n",
    "    \n",
    "    \n",
    "    # Loop through the data for subtraction\n",
    "    for i in data_text.keys():\n",
    "        sub[i + \"- sub\"] = one_to_one_dif(data_text[i], data_ref[i], subject_text)\n",
    "        avg_sub[i + \"- avg sub\"] = one_to_many_dif(data_text[i], data_ref[i], subject_text)\n",
    "    \n",
    "    # Creating dataframe with multi-sourced features\n",
    "    df['statement'] = state_text\n",
    "    df['label'] = label_text\n",
    "    df['subject'] = subject_text\n",
    "    \n",
    "    # Concatenate the text, reference and features, then drop null values\n",
    "    df1 = pd.concat([df, data_text, sub, avg_sub], axis = 1)\n",
    "    df1 = df1.dropna(how = 'any')\n",
    "    \n",
    "    return df1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[\\'count_unique\\', \\'sentiment_score\\', \\'count_uppercase\\', \\'num_\"\\', \\'num_;\\', \\'count_sent\\', \\'count_word\\', \\'count_char_per_word\\', \\'num_(\\', \\'num_$\\', \\'num_?\\', \\'num_-\\', \\'count_month\\', \\'num_,\\', \\'num_:\\', \\'num_%\\', \\'count_char\\', \\'num_.\\', \\'num_!\\'] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-daad4970cb49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmulti_source_FE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-d8668dd27ae0>\u001b[0m in \u001b[0;36mmulti_source_FE\u001b[1;34m(text, ref)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0msubject_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'subject'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mdata_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mdata_ref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0mfeature_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2999\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3000\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3001\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3002\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3003\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[0;32m   1283\u001b[0m                 \u001b[1;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"raise_missing\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_setter\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m         self._validate_read_indexer(\n\u001b[1;32m-> 1092\u001b[1;33m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1093\u001b[0m         )\n\u001b[0;32m   1094\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1183\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"loc\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} not in index\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnot_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[1;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '[\\'count_unique\\', \\'sentiment_score\\', \\'count_uppercase\\', \\'num_\"\\', \\'num_;\\', \\'count_sent\\', \\'count_word\\', \\'count_char_per_word\\', \\'num_(\\', \\'num_$\\', \\'num_?\\', \\'num_-\\', \\'count_month\\', \\'num_,\\', \\'num_:\\', \\'num_%\\', \\'count_char\\', \\'num_.\\', \\'num_!\\'] not in index'"
     ]
    }
   ],
   "source": [
    "multi_source_FE(df, ref_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
