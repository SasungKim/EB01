{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB Accuracy:  0.4648776637726914\n",
      "GNB F1 Score:  0.31376518218623484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x550 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x550 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighbors Accuracy:  0.5430149960536701\n",
      "KNeighbors F1 Score:  0.6063902107409924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x550 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x550 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[434 119]\n",
      " [559 155]]\n",
      "[[242 311]\n",
      " [268 446]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This program figures out if a news is fake or not.\n",
    "\n",
    "At the beginning, the program grab the news contents and labels (Fake or Not Fake) from the given dataset and extract the\n",
    "features from the news contents. Then, it saves extracted data and features in a file.\n",
    "With the data and features, it will decide if the news is fake or not fake using machine learning algorithms (GaussianNB \n",
    "and KNeighbors Classifier are used in this program)\n",
    "\n",
    "Features used in this program are:\n",
    "1. Number of Characters\n",
    "2. Number of Words\n",
    "3. Number of Verbs\n",
    "4. Number of Nouns\n",
    "5. Number of Sentence\n",
    "6. Average Number of Words per Sentence\n",
    "7. Average Number of Characters in Words\n",
    "8. Number of Question Marks\n",
    "9. Percentage of Subjective Verbs\n",
    "10. Percentage of Passive Voice\n",
    "11. Percentage of Positive Words\n",
    "12. Percentage of Negative Words\n",
    "13. Lexical Diversity: Unique Words or Terms\n",
    "14. Typographical Error Ration: Misspelled Words\n",
    "15. Causation Terms\n",
    "\n",
    "How to use: First, place positive-words.txt and negative-words.txt files in word_sentiment folder and put\n",
    "            .csv dataset in liar_dataset folder in datasets folder and put the name of the .csv file\n",
    "            in argument space of setup_data function. Next, use feature_extraction function with the result of\n",
    "            setup_data function to extract the features and save the data in the desired file. \n",
    "            \n",
    "            * The original dataset must include the labels with true and false in second column, and news\n",
    "            contents (statements) in third columns.\n",
    "            \n",
    "Date: Jan 20th, 2020\n",
    "Programmed by: Sasung Kim\n",
    "\n",
    "'''\n",
    "\n",
    "# Importing Required modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#########################################################################################################\n",
    "# Create the lists of the words that used for feature extraction\n",
    "\n",
    "positive_list = list()\n",
    "negative_list = list()\n",
    "\n",
    "# Gather the positive and negative word lists from the text file\n",
    "with open('./word_sentiment/positive-words.txt') as p:\n",
    "    for line in p:\n",
    "        val = line.split()\n",
    "        positive_list.append(val)\n",
    "        \n",
    "with open('./word_sentiment/negative-words.txt') as n:\n",
    "    for line in n:\n",
    "        val = line.split()\n",
    "        negative_list.append(val)\n",
    "\n",
    "subjective_list = list(['am ', 'are ', 'is ', 'was ', 'were ', 'be ', 'been '])\n",
    "causation_list = list(['led to ', 'because ', 'cause ', 'reason ', 'explanation ', 'so '])\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "# From the dataset, isolate the required data (statement and label in this program) and return as pandas Series\n",
    "# If the user needs the required data as a file, uncomment def with new_file as argument and comment the other one\n",
    "\n",
    "#def setup_data(file_name, new_file):\n",
    "def setup_data(file_name):\n",
    "    headernames = ['ID', 'Label', 'Statement', 'subject', 'speaker', 'sp_job', 'state info', 'party affiliation',\n",
    "                   'barely true', 'false', 'half true', 'mostly ture', 'pants on fire', 'context']\n",
    "    data = pd.read_csv('./datasets/liar_dataset/' + file_name, delimiter='\\t', names=headernames)\n",
    "    \n",
    "    # Check for the raw data from original dataset\n",
    "    # print (list(data.columns.values))\n",
    "    # print(data.tail(1))\n",
    "\n",
    "    Statement = data[\"Statement\"]\n",
    "    Label = data[\"Label\"]\n",
    "    # Check if the required data are correctly input\n",
    "    # print (Statement)\n",
    "    # print (Label)\n",
    "\n",
    "    # Convert the labels to Fake or Not Fake\n",
    "    Label = Label.replace(\"true\", \"Not Fake\")\n",
    "    Label = Label.replace(\"half-true\", \"Not Fake\")\n",
    "    Label = Label.replace(\"mostly-true\", \"Not Fake\")\n",
    "    Label = Label.replace(\"false\", \"Fake\")\n",
    "    Label = Label.replace(\"pants-fire\", \"Fake\")\n",
    "    Label = Label.replace(\"barely-true\", \"Fake\")\n",
    "\n",
    "    # Check the converted labels\n",
    "    # print(Label)\n",
    "\n",
    "    # Create a new pandas Series with statements and labels \n",
    "    new = pd.concat([Statement, Label], axis=1)\n",
    "    \n",
    "    # Check if the new pandas Series correctly created \n",
    "    # print(new.head(1))\n",
    "\n",
    "    # This part is for create a file to save the new pandas Series only with Statements and Labels from the dataset \n",
    "    # To use this part, uncommend this part, change the part where using this funciton with new file name as second argument and \n",
    "    # add one more argument, 'new_file'\n",
    "    \n",
    "    #new_header = ['Statement', 'Labels']\n",
    "    #with open(new_file, 'w') as f:\n",
    "    #    writer = csv.DictWriter(f, fieldnames=new_header)\n",
    "    #    writer.writeheader()\n",
    "    #new.to_csv(new_file)\n",
    "\n",
    "    return new\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "# Not required for right now, but may used in later for more efficient tockenization and feature extraction\n",
    "# vectorizer = CountVectorizer()\n",
    "# train_count = vectorizer.fit_transform(data[\"statement\"].values)\n",
    "\n",
    "# Check for vectorizer\n",
    "# print(train_count.shape)\n",
    "# print(vectorizer.vocabulary_)\n",
    "# print(vectorizer.get_feature_names())\n",
    "\n",
    "# Input: String ('str')\n",
    "# Description: Tockenize and tag input with nltk universal tag\n",
    "# Return: tags of each tockens - String ('tagged')\n",
    "# Tockenize and tag input String with nltk universal tag and return the tags for each words in the String\n",
    "\n",
    "def tagging_univ(str):\n",
    "    text = nltk.word_tokenize(str)\n",
    "    tagged = nltk.pos_tag(text, tagset='universal')\n",
    "    return tagged\n",
    "\n",
    "# Input: String('str')\n",
    "# Description: Tockenize and tag input with nltk non-universal tag\n",
    "# Return: tags of each tockens - String ('tagged')\n",
    "\n",
    "def tagging_nuniv(str):\n",
    "    text = nltk.word_tokenize(str)\n",
    "    tagged = nltk.pos_tag(text)\n",
    "    return tagged\n",
    "\n",
    "# Input: String('str')\n",
    "# Description: Count the number of characters in input\n",
    "# Return: Character count - int ('count')\n",
    "\n",
    "def count_char(str):\n",
    "    no_space = str.replace(\" \", \"\")\n",
    "    count = len(no_space)\n",
    "    return count\n",
    "\n",
    "# Input: String('str')\n",
    "# Description: Count the number of words in input\n",
    "# Return: Word count - int ('count')\n",
    "\n",
    "def count_word(str):\n",
    "    count = len(str.split())\n",
    "    return count\n",
    "\n",
    "# Input: String ('tags'), int(verb_count)\n",
    "# Description: Count the number of verbs in input \n",
    "# Return: Verb count - int ('verb_count')\n",
    "\n",
    "def count_verb(tags, verb_count):\n",
    "    if (tags == 'VERB'):\n",
    "        verb_count += 1\n",
    "    return verb_count\n",
    "\n",
    "# Input: String('tags'), int('noun_count')\n",
    "# Description: Count the number of nouns in input\n",
    "# Return: Noun count - int ('noun_count')\n",
    "\n",
    "def count_noun(tags, noun_count):\n",
    "    if (tags == 'NOUN'):\n",
    "        noun_count += 1\n",
    "    return noun_count\n",
    "\n",
    "# Input: String ('str')\n",
    "# Description: Count the number of sentences by counting number of period(.)\n",
    "# Return: Sentence count - int ('sentence')\n",
    "\n",
    "def count_sent(str):\n",
    "    sentence = len(str.split('.'))\n",
    "    return sentence\n",
    "\n",
    "# Input: String ('states')\n",
    "# Description: Count the number of characters in each word in input and average the number of characters per word\n",
    "# Return: Average number of characters: float ('avg')\n",
    "\n",
    "def count_char_per_word(states):\n",
    "    word = []\n",
    "    word.append(states.split())\n",
    "    char_per_word = list()\n",
    "    for elements in word:\n",
    "        for char in elements:\n",
    "            c_in_w_count = len(char)\n",
    "            char_per_word.append(c_in_w_count)\n",
    "    # char_per_word_list.append()\n",
    "    avg = sum(char_per_word) / len(char_per_word)\n",
    "    char_per_word.clear()\n",
    "    return avg\n",
    "\n",
    "# Input: String ('tagged'), list ('list'), int ('int')\n",
    "# Description: Count the number of common words between input String and input list\n",
    "# Return: Count of common words - int ('int')\n",
    "# Count and return the number of common words between input String ('tagged') and input list\n",
    "\n",
    "def check_common(tagged, list, int):\n",
    "    for elements in list:\n",
    "        if tagged in elements:\n",
    "            int += 1\n",
    "            return int\n",
    "    return int\n",
    "\n",
    "# Input: list ('subjective_list'), int ('verb_count'), String ('states')\n",
    "# Description: Count the number of subjective words and avarage it from number of verbs in input\n",
    "# Return: Average of subjective words - float ('percent_sub')\n",
    "\n",
    "def count_sub(subjective_list, verb_count, states):\n",
    "    percent_sub = 0\n",
    "    for ele in subjective_list:\n",
    "        sub_count = states.count(ele)\n",
    "    if (verb_count > 0):\n",
    "        percent_sub = sub_count / verb_count * 100\n",
    "    return percent_sub\n",
    "\n",
    "# Input: String ('word'), String ('tag'), list ('subjective_list'), int ('sent_count')\n",
    "# Description: Count the number of passive voice and avarge it from number of sentence (sent_count)\n",
    "# Return: Average of passive voiced sentences - float ('result')\n",
    "\n",
    "def count_passive(word, tag, subjective_list, sent_count):\n",
    "    percent_sub = 0\n",
    "    counter = 0\n",
    "    for ele in subjective_list:\n",
    "        if (word.count(ele) > 0 and tag == \"VBN\"):\n",
    "            counter += 1\n",
    "    result = counter / sent_count * 100\n",
    "    return result\n",
    "\n",
    "# Input: String ('states')\n",
    "# Description: Count the words that introduced only once in input\n",
    "# Return: Count of unique words - int ('unique_count')\n",
    "\n",
    "def count_unique(states):\n",
    "    words = states.split(' ')\n",
    "    c = Counter(words)\n",
    "    unique = [w for w in words if c[w] == 1]\n",
    "    unique_counter = len(unique)\n",
    "    return unique_counter\n",
    "\n",
    "# Input: String ('tag'), int ('typo_count')\n",
    "# Description: Count the number of words with nltk universal tag is \"X\" (misspelled)\n",
    "# Return: Count of misspelled words - int ('typo_count')\n",
    "\n",
    "def count_typo(tag, typo_count):\n",
    "    if (tag == \"X\"):\n",
    "        typo_count += 1\n",
    "    return typo_count\n",
    "\n",
    "# Input: list ('causation_list'), String ('states')\n",
    "# Description: Count the number of common words in list input and String input\n",
    "# Return: Count of common words - int ('cause_count')\n",
    "\n",
    "def count_cause(causation_list, states):\n",
    "    for ele in causation_list:\n",
    "        cause_count = states.count(ele)\n",
    "    return cause_count\n",
    "\n",
    "# Input: panda Series ('data'), String ('save_file_name')\n",
    "# Description: First, this function creates lists for each features and extract the features using statements in dataset and\n",
    "#              above functions. Next, it creates a large pandas Series that consist of news contents, labels and extracted \n",
    "#              features. Finally, it save the final pandas Series in a file to make easier to examine the result (do not need\n",
    "#              to rerun the program or change the code to check raw data)\n",
    "# Return: Pandas Series consist of news contents, labels and extracted features count - pandas Series ('new')\n",
    "\n",
    "def feature_extract(data, save_file_name):\n",
    "    # define the news contents and labels from the dataset\n",
    "    state = data.Statement\n",
    "    label = data.Label\n",
    "\n",
    "    # create lists for storing the counters\n",
    "    char_count_list = list()\n",
    "    word_count_list = list()\n",
    "    verb_count_list = list()\n",
    "    noun_count_list = list()\n",
    "    sent_count_list = list()\n",
    "    words_per_sent_list = list()\n",
    "    char_per_word_list = list()\n",
    "    quest_count_list = list()\n",
    "    sub_count_list = list()\n",
    "    pass_count_list = list()\n",
    "    pos_count_list = list()\n",
    "    neg_count_list = list()\n",
    "    unique_count_list = list()\n",
    "    typo_count_list = list()\n",
    "    cause_count_list = list()\n",
    "    word = list()\n",
    "\n",
    "    # loop for checking each new contents in dataset\n",
    "    for states in state:\n",
    "        \n",
    "        # reset the counters for each news contents\n",
    "        w_in_s_count = 0\n",
    "        c_in_w_count = 0\n",
    "        verb_count = 0\n",
    "        noun_count = 0\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        percent_pos = 0\n",
    "        percent_neg = 0\n",
    "        unique_count = 0\n",
    "        sent_counts = 0\n",
    "        typo_count = 0\n",
    "\n",
    "        # Tockenization and tagging with nltk universal and non-universal tag systems (tagged = universal, tagged_nu = non-universal)\n",
    "        tagged = tagging_univ(states)\n",
    "        tagged_nu = tagging_nuniv(states)\n",
    "        \n",
    "        # Check the tags of each news contents\n",
    "        # print(tagged)\n",
    "        # print(tagged_nu)\n",
    "\n",
    "        # Extract the features and append the results in the list. Commented lines with print() functions are for testing\n",
    "        \n",
    "        # 1. Number of Characters\n",
    "        char_count = count_char(states)\n",
    "        char_count_list.append(char_count)\n",
    "\n",
    "        # 2. Nubmer of Words\n",
    "        word_count = count_word(states)\n",
    "        word_count_list.append(word_count)\n",
    "\n",
    "        # 3. Number of Verbs\n",
    "        for tag in tagged:\n",
    "            verb_count = count_verb(tag[1], verb_count)\n",
    "        verb_count_list.append(verb_count)\n",
    "\n",
    "        # 4. Number of Nouns\n",
    "        for tag in tagged:\n",
    "            noun_count = count_noun(tag[1], noun_count)\n",
    "        noun_count_list.append(noun_count)\n",
    "        # print(noun_count)\n",
    "\n",
    "        # 5. Number of Sentence\n",
    "        sent_count = count_sent(states)\n",
    "        sent_count_list.append(sent_count)\n",
    "        # print(sent_count)\n",
    "\n",
    "        # 6. Average number of words per sentence\n",
    "        sent = [len(l.split()) for l in re.split(r'[?!.]', states) if l.strip()]\n",
    "        w_in_s_count = (sum(sent) / len(sent))\n",
    "        words_per_sent_list.append(w_in_s_count)\n",
    "        # print(w_in_s_count)\n",
    "\n",
    "        # 7. Average number of characters per word\n",
    "        c_in_w_count = count_char_per_word(states)\n",
    "        char_per_word_list.append(c_in_w_count)\n",
    "        # print(c_in_w_count)\n",
    "\n",
    "        # 8. Number of question marks\n",
    "        quest_count = states.count(\"?\")\n",
    "        quest_count_list.append(quest_count)\n",
    "\n",
    "        # 9. Percentage of subjective verbs - am/are/is/etc\n",
    "        percent_sub = count_sub(subjective_list, verb_count, states)\n",
    "        sub_count_list.append(percent_sub)\n",
    "\n",
    "        # 10. Percentage of passive voice - am/are/is && past participate\n",
    "        for tag in tagged_nu:\n",
    "            passive_percent = count_passive(tag[0], tag[1], subjective_list, sent_count)\n",
    "        pass_count_list.append(passive_percent)\n",
    "\n",
    "        # 11. Percentage of positive words\n",
    "        for tag in tagged:\n",
    "            pos_count = check_common(tag[0], positive_list, pos_count)\n",
    "            percent_pos = pos_count / word_count * 100\n",
    "        pos_count_list.append(percent_pos)\n",
    "\n",
    "        # 12. Percentage of negative words\n",
    "        for tag in tagged:\n",
    "            neg_count = check_common(tag[0], negative_list, neg_count)\n",
    "            percent_neg = neg_count / word_count * 100\n",
    "        neg_count_list.append(percent_neg)\n",
    "\n",
    "        # 13. Lexical diversity: unique words or terms\n",
    "        unique_count = count_unique(states)\n",
    "        unique_count_list.append(unique_count)\n",
    "\n",
    "        # 14. Typographical error ratio: misspelled words\n",
    "        for tag in tagged:\n",
    "            typo_count = count_typo(tag[1], typo_count)\n",
    "        typo_count_list.append(typo_count)\n",
    "\n",
    "        # 15. Causation terms\n",
    "        cause_count = count_cause(causation_list, states)\n",
    "        cause_count_list.append(cause_count)\n",
    "\n",
    "    # Convert lists into pandas Series\n",
    "    \n",
    "    first = pd.Series(char_count_list)\n",
    "    second = pd.Series(word_count_list)\n",
    "    third = pd.Series(verb_count_list)\n",
    "    fourth = pd.Series(noun_count_list)\n",
    "    fifth = pd.Series(sent_count_list)\n",
    "    sixth = pd.Series(words_per_sent_list)\n",
    "    seventh = pd.Series(char_per_word_list)\n",
    "    eighth = pd.Series(quest_count_list)\n",
    "    nineth = pd.Series(sub_count_list)\n",
    "    tenth = pd.Series(pass_count_list)\n",
    "    eleventh = pd.Series(pos_count_list)\n",
    "    twelveth = pd.Series(neg_count_list)\n",
    "    thirteenth = pd.Series(unique_count_list)\n",
    "    fourteenth = pd.Series(typo_count_list)\n",
    "    fifteenth = pd.Series(cause_count_list)\n",
    "\n",
    "    # Concatenate all feature Series, news contents and labels into one pandas Series\n",
    "    new = pd.concat([state, label, first, second, third, fourth, fifth, sixth, seventh, eighth, nineth, tenth,\n",
    "                     eleventh, twelveth, thirteenth, fourteenth, fifteenth], axis=1)\n",
    "    \n",
    "    # Test for the final Series\n",
    "    # print(new.head(3))\n",
    "\n",
    "    # Name the header of the data\n",
    "    new_header = ['Statement', 'Labels', '# of Characters', '# of Words', '# of Verbs', '# of Noun', '# of Sentence',\n",
    "                  'Average # of Words per Sentence', 'Average # of Characters per Words', '# of Question Marks',\n",
    "                  '% of Subjective Verbs', '% of Passive Voice', '% of Positive Words', '% of Negative Words',\n",
    "                  '# of Unique Wrods/Terms', '# of Misspelled Words', '# of Causation Terms']\n",
    "    \n",
    "    # Save the data in a file\n",
    "    \n",
    "    with open(save_file_name, 'w') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=new_header)\n",
    "        writer.writeheader()\n",
    "\n",
    "    new.to_csv(save_file_name, header=new_header)\n",
    "    # print(new)\n",
    "    return new\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# Data Extraction\n",
    "# From the dataset file, gather the news contents and labels only\n",
    "# *If the file only for news contents and labels, uncomment the first three lines and comment the last three lines\n",
    "\n",
    "#train_data = setup_data('train.tsv', 'train.csv')\n",
    "#test_data = setup_data('test.tsv', 'test.csv')\n",
    "#valid_data = setup_data('valid.tsv', 'valid.csv')\n",
    "train_data = setup_data('train.tsv')\n",
    "test_data = setup_data('test.tsv')\n",
    "valid_data = setup_data('valid.tsv')\n",
    "\n",
    "# Check for shape and contents of isolated data\n",
    "# print(train_data.shape)\n",
    "# print(test_data.shape)\n",
    "# print (valid_data.shape)\n",
    "# print (train_data.head(5))\n",
    "# print (test_data.head(5))\n",
    "# print (valid_data.head(5))\n",
    "\n",
    "# Feature Extraction\n",
    "\n",
    "train_data_final = feature_extract(train_data, 'train_data.csv')\n",
    "test_data_final = feature_extract(test_data, 'test_data.csv')\n",
    "valid_data_final = feature_extract(valid_data, 'valid_data.csv')\n",
    "\n",
    "\n",
    "# print(valid_data_final)\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "# Label Encoding\n",
    "LE = preprocessing.LabelEncoder()\n",
    "# Fake = 0, Not Fake = 1\n",
    "train_data_final['Label'] = LE.fit_transform(train_data_final['Label'])\n",
    "test_data_final['Label'] = LE.fit_transform(test_data_final['Label'])\n",
    "valid_data_final['Label'] = LE.fit_transform(valid_data_final['Label'])\n",
    "\n",
    "# Model Design & Evaluating\n",
    "\n",
    "# Distinguish input (news content and features) and target (label) data for each dataset\n",
    "cols = [col for col in train_data_final.columns if col not in ['Unnamed: 0', 'Label', 'Statement']]\n",
    "X_train = train_data_final[cols]\n",
    "Y_train = train_data_final['Label']\n",
    "X_test = test_data_final[cols]\n",
    "Y_test = test_data_final['Label']\n",
    "X_valid = valid_data_final[cols]\n",
    "Y_valid = valid_data_final['Label']\n",
    "\n",
    "# Check for distinguished data\n",
    "#print(X_test.head(n=5))\n",
    "#print(Y_train.head(n=5))\n",
    "\n",
    "# GaussianNB\n",
    "\n",
    "# Create a pipeline with StandardScaler and GaussianNB\n",
    "pipeline = make_pipeline(preprocessing.StandardScaler(), GaussianNB(priors=None))\n",
    "\n",
    "# Fit the GaussianNB model with training set and use for predict the test set\n",
    "pred = pipeline.fit(X_train, Y_train).predict(X_test)\n",
    "\n",
    "# Print out the GaussianNB result (accuracy and f1 score) \n",
    "print(\"GNB Accuracy: \", accuracy_score(Y_test, pred))\n",
    "print(\"GNB F1 Score: \", f1_score(Y_test, pred))\n",
    "\n",
    "# Output the result (percision, recall and f1 score) with test dataset as diagram\n",
    "visualizer1 = ClassificationReport(pipeline, classes=['Fake', 'Not Fake'])\n",
    "visualizer1.fit(X_train, Y_train)\n",
    "visualizer1.score(X_test, Y_test)\n",
    "visualizer1.show()\n",
    "\n",
    "# Output the result (percision, recall and f1 score) with validation dataset as diagram\n",
    "visualizer1_valid = ClassificationReport(pipeline, classes=['Fake', 'Not Fake'])\n",
    "visualizer1_valid.fit(X_train, Y_train)\n",
    "visualizer1_valid.score(X_valid, Y_valid)\n",
    "visualizer1_valid.show()\n",
    "\n",
    "# KNeighbors Classifier\n",
    "\n",
    "# Set the classifier with number of neighbors = 3\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "# Fit the model with training set and predict the test set\n",
    "pred1 = neigh.fit(X_train, Y_train).predict(X_test)\n",
    "\n",
    "# Print out the KNeighborsClassifier result (accuracy and f1 score) \n",
    "print(\"KNeighbors Accuracy: \", accuracy_score(Y_test, pred1))\n",
    "print(\"KNeighbors F1 Score: \", f1_score(Y_test, pred1))\n",
    "\n",
    "# Output the result (percision, recall and f1 score) with test dataset as diagram\n",
    "visualizer3 = ClassificationReport(neigh, classes=['Fake', 'Not Fake'])\n",
    "visualizer3.fit(X_train, Y_train)\n",
    "visualizer3.score(X_test, Y_test)\n",
    "visualizer3.show()\n",
    "\n",
    "# Output the result (percision, recall and f1 score) with validation dataset as diagram\n",
    "visualizer3_valid = ClassificationReport(neigh, classes=['Fake', 'Not Fake'])\n",
    "visualizer3_valid.fit(X_train, Y_train)\n",
    "visualizer3_valid.score(X_valid, Y_valid)\n",
    "visualizer3_valid.show()\n",
    "\n",
    "# Create confusion matrix with result from GaussianNB\n",
    "conf_matrix1 = confusion_matrix(Y_test, pred)\n",
    "print(conf_matrix1)\n",
    "\n",
    "# Create confusion matrix with result from KNeighbors Classifier\n",
    "conf_matrix2 = confusion_matrix(Y_test, pred1)\n",
    "print(conf_matrix2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This program figures out if a news is fake or not.\n",
    "\n",
    "At the beginning, the program grab the news contents and labels (Fake or Not Fake) from the given dataset and extract the\n",
    "features from the news contents. Then, it saves extracted data and features in a file.\n",
    "With the data and features, it will decide if the news is fake or not fake using machine learning algorithms (GaussianNB \n",
    "and KNeighbors Classifier are used in this program)\n",
    "\n",
    "Features used in this program are:\n",
    "1. Number of Characters\n",
    "2. Number of Words\n",
    "3. Number of Verbs\n",
    "4. Number of Nouns\n",
    "5. Number of Sentence\n",
    "6. Average Number of Words per Sentence\n",
    "7. Average Number of Characters in Words\n",
    "8. Number of Question Marks\n",
    "9. Percentage of Subjective Verbs\n",
    "10. Percentage of Passive Voice\n",
    "11. Percentage of Positive Words\n",
    "12. Percentage of Negative Words\n",
    "13. Lexical Diversity: Unique Words or Terms\n",
    "14. Typographical Error Ration: Misspelled Words\n",
    "15. Causation Terms\n",
    "\n",
    "How to use: First, place positive-words.txt and negative-words.txt files in word_sentiment folder and put\n",
    "            .csv dataset in liar_dataset folder in datasets folder and put the name of the .csv file\n",
    "            in argument space of setup_data function. Next, use feature_extraction function with the result of\n",
    "            setup_data function to extract the features and save the data in the desired file. \n",
    "            \n",
    "            * The original dataset must include the labels with true and false in second column, and news\n",
    "            contents (statements) in third columns.\n",
    "\n",
    "'''\n",
    "\n",
    "# Importing Required modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#########################################################################################################\n",
    "# Create the lists of the words that used for feature extraction\n",
    "\n",
    "positive_list = list()\n",
    "negative_list = list()\n",
    "\n",
    "# Gather the positive and negative word lists from the text file\n",
    "with open('./word_sentiment/positive-words.txt') as p:\n",
    "    for line in p:\n",
    "        val = line.split()\n",
    "        positive_list.append(val)\n",
    "        \n",
    "with open('./word_sentiment/negative-words.txt') as n:\n",
    "    for line in n:\n",
    "        val = line.split()\n",
    "        negative_list.append(val)\n",
    "\n",
    "subjective_list = list(['am ', 'are ', 'is ', 'was ', 'were ', 'be ', 'been '])\n",
    "causation_list = list(['led to ', 'because ', 'cause ', 'reason ', 'explanation ', 'so '])\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "# From the dataset, isolate the required data (statement and label in this program) and return as pandas Series\n",
    "# If the user needs the required data as a file, uncomment def with new_file as argument and comment the other one\n",
    "\n",
    "#def setup_data(file_name, new_file):\n",
    "def setup_data(file_name):\n",
    "    headernames = ['ID', 'Label', 'Statement', 'subject', 'speaker', 'sp_job', 'state info', 'party affiliation',\n",
    "                   'barely true', 'false', 'half true', 'mostly ture', 'pants on fire', 'context']\n",
    "    data = pd.read_csv('./datasets/liar_dataset/' + file_name, delimiter='\\t', names=headernames)\n",
    "    \n",
    "    # Check for the raw data from original dataset\n",
    "    # print (list(data.columns.values))\n",
    "    # print(data.tail(1))\n",
    "\n",
    "    Statement = data[\"Statement\"]\n",
    "    Label = data[\"Label\"]\n",
    "    # Check if the required data are correctly input\n",
    "    # print (Statement)\n",
    "    # print (Label)\n",
    "\n",
    "    # Convert the labels to Fake or Not Fake\n",
    "    Label = Label.replace(\"true\", \"Not Fake\")\n",
    "    Label = Label.replace(\"half-true\", \"Not Fake\")\n",
    "    Label = Label.replace(\"mostly-true\", \"Not Fake\")\n",
    "    Label = Label.replace(\"false\", \"Fake\")\n",
    "    Label = Label.replace(\"pants-fire\", \"Fake\")\n",
    "    Label = Label.replace(\"barely-true\", \"Fake\")\n",
    "\n",
    "    # Check the converted labels\n",
    "    # print(Label)\n",
    "\n",
    "    # Create a new pandas Series with statements and labels \n",
    "    new = pd.concat([Statement, Label], axis=1)\n",
    "    \n",
    "    # Check if the new pandas Series correctly created \n",
    "    # print(new.head(1))\n",
    "\n",
    "    # This part is for create a file to save the new pandas Series only with Statements and Labels from the dataset \n",
    "    # To use this part, uncommend this part, change the part where using this funciton with new file name as second argument and \n",
    "    # add one more argument, 'new_file'\n",
    "    \n",
    "    #new_header = ['Statement', 'Labels']\n",
    "    #with open(new_file, 'w') as f:\n",
    "    #    writer = csv.DictWriter(f, fieldnames=new_header)\n",
    "    #    writer.writeheader()\n",
    "    #new.to_csv(new_file)\n",
    "\n",
    "    return new\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "# Not required for right now, but may used in later for more efficient tockenization and feature extraction\n",
    "# vectorizer = CountVectorizer()\n",
    "# train_count = vectorizer.fit_transform(data[\"statement\"].values)\n",
    "\n",
    "# Check for vectorizer\n",
    "# print(train_count.shape)\n",
    "# print(vectorizer.vocabulary_)\n",
    "# print(vectorizer.get_feature_names())\n",
    "\n",
    "# Input: String ('str')\n",
    "# Description: Tockenize and tag input with nltk universal tag\n",
    "# Return: tags of each tockens - String ('tagged')\n",
    "# Tockenize and tag input String with nltk universal tag and return the tags for each words in the String\n",
    "\n",
    "def tagging_univ(str):\n",
    "    text = nltk.word_tokenize(str)\n",
    "    tagged = nltk.pos_tag(text, tagset='universal')\n",
    "    return tagged\n",
    "\n",
    "# Input: String('str')\n",
    "# Description: Tockenize and tag input with nltk non-universal tag\n",
    "# Return: tags of each tockens - String ('tagged')\n",
    "\n",
    "def tagging_nuniv(str):\n",
    "    text = nltk.word_tokenize(str)\n",
    "    tagged = nltk.pos_tag(text)\n",
    "    return tagged\n",
    "\n",
    "# Input: String('str')\n",
    "# Description: Count the number of characters in input\n",
    "# Return: Character count - int ('count')\n",
    "\n",
    "def count_char(str):\n",
    "    no_space = str.replace(\" \", \"\")\n",
    "    count = len(no_space)\n",
    "    return count\n",
    "\n",
    "# Input: String('str')\n",
    "# Description: Count the number of words in input\n",
    "# Return: Word count - int ('count')\n",
    "\n",
    "def count_word(str):\n",
    "    count = len(str.split())\n",
    "    return count\n",
    "\n",
    "# Input: String ('tags'), int(verb_count)\n",
    "# Description: Count the number of verbs in input \n",
    "# Return: Verb count - int ('verb_count')\n",
    "\n",
    "def count_verb(tags, verb_count):\n",
    "    if (tags == 'VERB'):\n",
    "        verb_count += 1\n",
    "    return verb_count\n",
    "\n",
    "# Input: String('tags'), int('noun_count')\n",
    "# Description: Count the number of nouns in input\n",
    "# Return: Noun count - int ('noun_count')\n",
    "\n",
    "def count_noun(tags, noun_count):\n",
    "    if (tags == 'NOUN'):\n",
    "        noun_count += 1\n",
    "    return noun_count\n",
    "\n",
    "# Input: String ('str')\n",
    "# Description: Count the number of sentences by counting number of period(.)\n",
    "# Return: Sentence count - int ('sentence')\n",
    "\n",
    "def count_sent(str):\n",
    "    sentence = len(str.split('.'))\n",
    "    return sentence\n",
    "\n",
    "# Input: String ('states')\n",
    "# Description: Count the number of characters in each word in input and average the number of characters per word\n",
    "# Return: Average number of characters: float ('avg')\n",
    "\n",
    "def count_char_per_word(states):\n",
    "    word = []\n",
    "    word.append(states.split())\n",
    "    char_per_word = list()\n",
    "    for elements in word:\n",
    "        for char in elements:\n",
    "            c_in_w_count = len(char)\n",
    "            char_per_word.append(c_in_w_count)\n",
    "    # char_per_word_list.append()\n",
    "    avg = sum(char_per_word) / len(char_per_word)\n",
    "    char_per_word.clear()\n",
    "    return avg\n",
    "\n",
    "# Input: String ('tagged'), list ('list'), int ('int')\n",
    "# Description: Count the number of common words between input String and input list\n",
    "# Return: Count of common words - int ('int')\n",
    "# Count and return the number of common words between input String ('tagged') and input list\n",
    "\n",
    "def check_common(tagged, list, int):\n",
    "    for elements in list:\n",
    "        if tagged in elements:\n",
    "            int += 1\n",
    "            return int\n",
    "    return int\n",
    "\n",
    "# Input: list ('subjective_list'), int ('verb_count'), String ('states')\n",
    "# Description: Count the number of subjective words and avarage it from number of verbs in input\n",
    "# Return: Average of subjective words - float ('percent_sub')\n",
    "\n",
    "def count_sub(subjective_list, verb_count, states):\n",
    "    percent_sub = 0\n",
    "    for ele in subjective_list:\n",
    "        sub_count = states.count(ele)\n",
    "    if (verb_count > 0):\n",
    "        percent_sub = sub_count / verb_count * 100\n",
    "    return percent_sub\n",
    "\n",
    "# Input: String ('word'), String ('tag'), list ('subjective_list'), int ('sent_count')\n",
    "# Description: Count the number of passive voice and avarge it from number of sentence (sent_count)\n",
    "# Return: Average of passive voiced sentences - float ('result')\n",
    "\n",
    "def count_passive(word, tag, subjective_list, sent_count):\n",
    "    percent_sub = 0\n",
    "    counter = 0\n",
    "    for ele in subjective_list:\n",
    "        if (word.count(ele) > 0 and tag == \"VBN\"):\n",
    "            counter += 1\n",
    "    result = counter / sent_count * 100\n",
    "    return result\n",
    "\n",
    "# Input: String ('states')\n",
    "# Description: Count the words that introduced only once in input\n",
    "# Return: Count of unique words - int ('unique_count')\n",
    "\n",
    "def count_unique(states):\n",
    "    words = states.split(' ')\n",
    "    c = Counter(words)\n",
    "    unique = [w for w in words if c[w] == 1]\n",
    "    unique_counter = len(unique)\n",
    "    return unique_counter\n",
    "\n",
    "# Input: String ('tag'), int ('typo_count')\n",
    "# Description: Count the number of words with nltk universal tag is \"X\" (misspelled)\n",
    "# Return: Count of misspelled words - int ('typo_count')\n",
    "\n",
    "def count_typo(tag, typo_count):\n",
    "    if (tag == \"X\"):\n",
    "        typo_count += 1\n",
    "    return typo_count\n",
    "\n",
    "# Input: list ('causation_list'), String ('states')\n",
    "# Description: Count the number of common words in list input and String input\n",
    "# Return: Count of common words - int ('cause_count')\n",
    "\n",
    "def count_cause(causation_list, states):\n",
    "    for ele in causation_list:\n",
    "        cause_count = states.count(ele)\n",
    "    return cause_count\n",
    "\n",
    "# Input: panda Series ('data'), String ('save_file_name')\n",
    "# Description: First, this function creates lists for each features and extract the features using statements in dataset and\n",
    "#              above functions. Next, it creates a large pandas Series that consist of news contents, labels and extracted \n",
    "#              features. Finally, it save the final pandas Series in a file to make easier to examine the result (do not need\n",
    "#              to rerun the program or change the code to check raw data)\n",
    "# Return: Pandas Series consist of news contents, labels and extracted features count - pandas Series ('new')\n",
    "\n",
    "def feature_extract(data, save_file_name):\n",
    "    # define the news contents and labels from the dataset\n",
    "    state = data.Statement\n",
    "    label = data.Label\n",
    "\n",
    "    # create lists for storing the counters\n",
    "    char_count_list = list()\n",
    "    word_count_list = list()\n",
    "    verb_count_list = list()\n",
    "    noun_count_list = list()\n",
    "    sent_count_list = list()\n",
    "    words_per_sent_list = list()\n",
    "    char_per_word_list = list()\n",
    "    quest_count_list = list()\n",
    "    sub_count_list = list()\n",
    "    pass_count_list = list()\n",
    "    pos_count_list = list()\n",
    "    neg_count_list = list()\n",
    "    unique_count_list = list()\n",
    "    typo_count_list = list()\n",
    "    cause_count_list = list()\n",
    "    word = list()\n",
    "\n",
    "    # loop for checking each new contents in dataset\n",
    "    for states in state:\n",
    "        \n",
    "        # reset the counters for each news contents\n",
    "        w_in_s_count = 0\n",
    "        c_in_w_count = 0\n",
    "        verb_count = 0\n",
    "        noun_count = 0\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        percent_pos = 0\n",
    "        percent_neg = 0\n",
    "        unique_count = 0\n",
    "        sent_counts = 0\n",
    "        typo_count = 0\n",
    "\n",
    "        # Tockenization and tagging with nltk universal and non-universal tag systems (tagged = universal, tagged_nu = non-universal)\n",
    "        tagged = tagging_univ(states)\n",
    "        tagged_nu = tagging_nuniv(states)\n",
    "        \n",
    "        # Check the tags of each news contents\n",
    "        # print(tagged)\n",
    "        # print(tagged_nu)\n",
    "\n",
    "        # Extract the features and append the results in the list. Commented lines with print() functions are for testing\n",
    "        \n",
    "        # 1. Number of Characters\n",
    "        char_count = count_char(states)\n",
    "        char_count_list.append(char_count)\n",
    "\n",
    "        # 2. Nubmer of Words\n",
    "        word_count = count_word(states)\n",
    "        word_count_list.append(word_count)\n",
    "\n",
    "        # 3. Number of Verbs\n",
    "        for tag in tagged:\n",
    "            verb_count = count_verb(tag[1], verb_count)\n",
    "        verb_count_list.append(verb_count)\n",
    "\n",
    "        # 4. Number of Nouns\n",
    "        for tag in tagged:\n",
    "            noun_count = count_noun(tag[1], noun_count)\n",
    "        noun_count_list.append(noun_count)\n",
    "        # print(noun_count)\n",
    "\n",
    "        # 5. Number of Sentence\n",
    "        sent_count = count_sent(states)\n",
    "        sent_count_list.append(sent_count)\n",
    "        # print(sent_count)\n",
    "\n",
    "        # 6. Average number of words per sentence\n",
    "        sent = [len(l.split()) for l in re.split(r'[?!.]', states) if l.strip()]\n",
    "        w_in_s_count = (sum(sent) / len(sent))\n",
    "        words_per_sent_list.append(w_in_s_count)\n",
    "        # print(w_in_s_count)\n",
    "\n",
    "        # 7. Average number of characters per word\n",
    "        c_in_w_count = count_char_per_word(states)\n",
    "        char_per_word_list.append(c_in_w_count)\n",
    "        # print(c_in_w_count)\n",
    "\n",
    "        # 8. Number of question marks\n",
    "        quest_count = states.count(\"?\")\n",
    "        quest_count_list.append(quest_count)\n",
    "\n",
    "        # 9. Percentage of subjective verbs - am/are/is/etc\n",
    "        percent_sub = count_sub(subjective_list, verb_count, states)\n",
    "        sub_count_list.append(percent_sub)\n",
    "\n",
    "        # 10. Percentage of passive voice - am/are/is && past participate\n",
    "        for tag in tagged_nu:\n",
    "            passive_percent = count_passive(tag[0], tag[1], subjective_list, sent_count)\n",
    "        pass_count_list.append(passive_percent)\n",
    "\n",
    "        # 11. Percentage of positive words\n",
    "        for tag in tagged:\n",
    "            pos_count = check_common(tag[0], positive_list, pos_count)\n",
    "            percent_pos = pos_count / word_count * 100\n",
    "        pos_count_list.append(percent_pos)\n",
    "\n",
    "        # 12. Percentage of negative words\n",
    "        for tag in tagged:\n",
    "            neg_count = check_common(tag[0], negative_list, neg_count)\n",
    "            percent_neg = neg_count / word_count * 100\n",
    "        neg_count_list.append(percent_neg)\n",
    "\n",
    "        # 13. Lexical diversity: unique words or terms\n",
    "        unique_count = count_unique(states)\n",
    "        unique_count_list.append(unique_count)\n",
    "\n",
    "        # 14. Typographical error ratio: misspelled words\n",
    "        for tag in tagged:\n",
    "            typo_count = count_typo(tag[1], typo_count)\n",
    "        typo_count_list.append(typo_count)\n",
    "\n",
    "        # 15. Causation terms\n",
    "        cause_count = count_cause(causation_list, states)\n",
    "        cause_count_list.append(cause_count)\n",
    "\n",
    "    # Convert lists into pandas Series\n",
    "    \n",
    "    first = pd.Series(char_count_list)\n",
    "    second = pd.Series(word_count_list)\n",
    "    third = pd.Series(verb_count_list)\n",
    "    fourth = pd.Series(noun_count_list)\n",
    "    fifth = pd.Series(sent_count_list)\n",
    "    sixth = pd.Series(words_per_sent_list)\n",
    "    seventh = pd.Series(char_per_word_list)\n",
    "    eighth = pd.Series(quest_count_list)\n",
    "    nineth = pd.Series(sub_count_list)\n",
    "    tenth = pd.Series(pass_count_list)\n",
    "    eleventh = pd.Series(pos_count_list)\n",
    "    twelveth = pd.Series(neg_count_list)\n",
    "    thirteenth = pd.Series(unique_count_list)\n",
    "    fourteenth = pd.Series(typo_count_list)\n",
    "    fifteenth = pd.Series(cause_count_list)\n",
    "\n",
    "    # Concatenate all feature Series, news contents and labels into one pandas Series\n",
    "    #new = pd.concat([state, label, first, second, third, fourth, fifth, sixth, seventh, eighth, nineth, tenth,\n",
    "    #                 eleventh, twelveth, thirteenth, fourteenth, fifteenth], axis=1)\n",
    "    \n",
    "    # Testing for each features\n",
    "    new = pd.concat([state, label, fifteenth], axis = 1)\n",
    "    \n",
    "    # Test for the final Series\n",
    "    # print(new.head(3))\n",
    "\n",
    "    # Name the header of the data\n",
    "    #new_header = ['Statement', 'Labels', '# of Characters', '# of Words', '# of Verbs', '# of Noun', '# of Sentence',\n",
    "    #              'Average # of Words per Sentence', 'Average # of Characters per Words', '# of Question Marks',\n",
    "    #              '% of Subjective Verbs', '% of Passive Voice', '% of Positive Words', '% of Negative Words',\n",
    "    #              '# of Unique Wrods/Terms', '# of Misspelled Words', '# of Causation Terms']\n",
    "    \n",
    "    new_header = ['Statement', 'Label', 'Test']\n",
    "    \n",
    "    # Save the data in a file\n",
    "    \n",
    "    with open(save_file_name, 'w') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=new_header)\n",
    "        writer.writeheader()\n",
    "\n",
    "    new.to_csv(save_file_name, header=new_header)\n",
    "    #print(new)\n",
    "    return new\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# Data Extraction\n",
    "# From the dataset file, gather the news contents and labels only\n",
    "# *If the file only for news contents and labels, uncomment the first three lines and comment the last three lines\n",
    "\n",
    "#train_data = setup_data('train.tsv', 'train.csv')\n",
    "#test_data = setup_data('test.tsv', 'test.csv')\n",
    "#valid_data = setup_data('valid.tsv', 'valid.csv')\n",
    "train_data = setup_data('train.tsv')\n",
    "test_data = setup_data('test.tsv')\n",
    "valid_data = setup_data('valid.tsv')\n",
    "\n",
    "# Check for shape and contents of isolated data\n",
    "# print(train_data.shape)\n",
    "# print(test_data.shape)\n",
    "# print (valid_data.shape)\n",
    "# print (train_data.head(5))\n",
    "# print (test_data.head(5))\n",
    "# print (valid_data.head(5))\n",
    "\n",
    "# Feature Extraction\n",
    "\n",
    "train_data_final = feature_extract(train_data, 'train_data.csv')\n",
    "test_data_final = feature_extract(test_data, 'test_data.csv')\n",
    "valid_data_final = feature_extract(valid_data, 'valid_data.csv')\n",
    "\n",
    "\n",
    "#print(valid_data_final.head(3))\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "# Label Encoding\n",
    "LE = preprocessing.LabelEncoder()\n",
    "# Fake = 0, Not Fake = 1\n",
    "train_data_final['Label'] = LE.fit_transform(train_data_final['Label'])\n",
    "test_data_final['Label'] = LE.fit_transform(test_data_final['Label'])\n",
    "valid_data_final['Label'] = LE.fit_transform(valid_data_final['Label'])\n",
    "\n",
    "# Model Design & Evaluating\n",
    "\n",
    "# Distinguish input (news content and features) and target (label) data for each dataset\n",
    "cols = [col for col in train_data_final.columns if col not in ['Unnamed: 0', 'Label', 'Statement']]\n",
    "X_train = train_data_final[cols]\n",
    "Y_train = train_data_final['Label']\n",
    "X_test = test_data_final[cols]\n",
    "Y_test = test_data_final['Label']\n",
    "X_valid = valid_data_final[cols]\n",
    "Y_valid = valid_data_final['Label']\n",
    "\n",
    "# Check for distinguished data\n",
    "#print(X_test.head(n=5))\n",
    "#print(Y_train.head(n=5))\n",
    "\n",
    "# GaussianNB\n",
    "\n",
    "# Create a pipeline with StandardScaler and GaussianNB\n",
    "pipeline = make_pipeline(preprocessing.StandardScaler(), GaussianNB(priors=None))\n",
    "\n",
    "# Fit the GaussianNB model with training set and use for predict the test set\n",
    "pred = pipeline.fit(X_train, Y_train).predict(X_test)\n",
    "\n",
    "# Print out the GaussianNB result (accuracy and f1 score) \n",
    "print(\"GNB Accuracy: \", accuracy_score(Y_test, pred))\n",
    "print(\"GNB F1 Score: \", f1_score(Y_test, pred))\n",
    "\n",
    "# Output the result (percision, recall and f1 score) with test dataset as diagram\n",
    "visualizer1 = ClassificationReport(pipeline, classes=['Fake', 'Not Fake'])\n",
    "visualizer1.fit(X_train, Y_train)\n",
    "visualizer1.score(X_test, Y_test)\n",
    "visualizer1.show()\n",
    "\n",
    "# Output the result (percision, recall and f1 score) with validation dataset as diagram\n",
    "visualizer1_valid = ClassificationReport(pipeline, classes=['Fake', 'Not Fake'])\n",
    "visualizer1_valid.fit(X_train, Y_train)\n",
    "visualizer1_valid.score(X_valid, Y_valid)\n",
    "visualizer1_valid.show()\n",
    "\n",
    "# KNeighbors Classifier\n",
    "\n",
    "# Set the classifier with number of neighbors = 3\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "# Fit the model with training set and predict the test set\n",
    "pred1 = neigh.fit(X_train, Y_train).predict(X_test)\n",
    "\n",
    "# Print out the KNeighborsClassifier result (accuracy and f1 score) \n",
    "print(\"KNeighbors Accuracy: \", accuracy_score(Y_test, pred1))\n",
    "print(\"KNeighbors F1 Score: \", f1_score(Y_test, pred1))\n",
    "\n",
    "# Output the result (percision, recall and f1 score) with test dataset as diagram\n",
    "visualizer3 = ClassificationReport(neigh, classes=['Fake', 'Not Fake'])\n",
    "visualizer3.fit(X_train, Y_train)\n",
    "visualizer3.score(X_test, Y_test)\n",
    "visualizer3.show()\n",
    "\n",
    "# Output the result (percision, recall and f1 score) with validation dataset as diagram\n",
    "visualizer3_valid = ClassificationReport(neigh, classes=['Fake', 'Not Fake'])\n",
    "visualizer3_valid.fit(X_train, Y_train)\n",
    "visualizer3_valid.score(X_valid, Y_valid)\n",
    "visualizer3_valid.show()\n",
    "\n",
    "# Create confusion matrix with result from GaussianNB\n",
    "conf_matrix1 = confusion_matrix(Y_test, pred)\n",
    "print(conf_matrix1)\n",
    "\n",
    "# Create confusion matrix with result from KNeighbors Classifier\n",
    "conf_matrix2 = confusion_matrix(Y_test, pred1)\n",
    "print(conf_matrix2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
