{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  source_id                      author  \\\n",
      "0       cnn  Shannon Liao, CNN Business   \n",
      "1       cnn       Michael Nedelman, CNN   \n",
      "\n",
      "                                               title  \\\n",
      "0  Apple temporarily closes all 42 stores in Chin...   \n",
      "1  New report on first US case of the virus detai...   \n",
      "\n",
      "                                         description  \\\n",
      "0  Apple is temporarily closing all of its stores...   \n",
      "1  Doctors have shared new details about the firs...   \n",
      "\n",
      "                                                 url content  \\\n",
      "0  https://www.cnn.com/2020/02/01/tech/apple-chin...    None   \n",
      "1  https://www.cnn.com/2020/01/31/health/washingt...    None   \n",
      "\n",
      "               pub_date  \n",
      "0  2020-02-01T20:52:44Z  \n",
      "1  2020-01-31T22:47:16Z  \n",
      "   source_id           author  \\\n",
      "0  the-verge   Nicole Wetsman   \n",
      "1  the-verge  Eleanor Cummins   \n",
      "\n",
      "                                               title  \\\n",
      "0  Universal coronavirus treatments could help tr...   \n",
      "1  The new coronavirus is not an excuse to be racist   \n",
      "\n",
      "                                         description  \\\n",
      "0  Coronaviruses caused SARS, MERS, and the curre...   \n",
      "1  The coronavirus — which originated in Wuhan, C...   \n",
      "\n",
      "                                                 url  \\\n",
      "0  https://www.theverge.com/2020/1/31/21114176/co...   \n",
      "1  https://www.theverge.com/2020/2/4/21121358/cor...   \n",
      "\n",
      "                                             content              pub_date  \n",
      "0  This wont be the last coronavirus to infect hu...  2020-01-31T14:00:00Z  \n",
      "1  A worker wears a surgical face mask as she ser...  2020-02-04T16:18:18Z  \n"
     ]
    }
   ],
   "source": [
    "# This program scraps a news about specific topic from newsapi.org API.\n",
    "# To change topic (or select topic), change q = '' part\n",
    "# To get specific news domain, use domains = '<news domain url>&'\n",
    "# Use page=#& to get more results\n",
    "\n",
    "# Author, title, description, url, content, publishing date, source ID \n",
    "\n",
    "# Import required modules\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "def news_api(topic, number, source, date, file_name):\n",
    "    # List creation\n",
    "    source_id = []\n",
    "    author = []\n",
    "    title = []\n",
    "    description = []\n",
    "    url = []\n",
    "    content = []\n",
    "    pub_date = []\n",
    "\n",
    "\n",
    "    # Read 100 news articles about coronavirus (20 articles per each page, 5 pages) and parse each data into corresponding lists\n",
    "    for i in range(1, int((number/20)+1)):\n",
    "\n",
    "        # News from CNN\n",
    "        api = ('https://newsapi.org/v2/everything?'\n",
    "                f'domains={source}&'\n",
    "                f'q={topic}&'\n",
    "               f'page={i}&'\n",
    "               f'from={date}&'\n",
    "               'sortBy=popularity&'\n",
    "               'apiKey=ddb22f3ef2b54abe8af228db83421424')\n",
    "\n",
    "        data = requests.get(api)\n",
    "\n",
    "        # Testing for data\n",
    "        #print(api.json()['articles'])\n",
    "    \n",
    "        for elements in data.json()['articles']:\n",
    "            source_id.append(elements['source']['id'])\n",
    "            author.append(elements['author'])\n",
    "            title.append(elements['title'])\n",
    "            description.append(elements['description'])\n",
    "            url.append(elements['url'])\n",
    "            content.append(elements['content'])\n",
    "            pub_date.append(elements['publishedAt'])\n",
    "\n",
    "            # *For CNN Only: check if the content is video and data does not related to the article\n",
    "            #if (elements['content'] == 'None' or elements['content'] == \"Chat with us in Facebook Messenger. Find out what's happening in the world as it unfolds.\"):\n",
    "            #    pass\n",
    "                #print(elements['description'])\n",
    "            #else:\n",
    "                #print (elements['content'])\n",
    "            #    pass\n",
    "\n",
    "    # Test for parsed data    \n",
    "    #print(author)\n",
    "\n",
    "    # Series Creation\n",
    "    source_id_ser = pd.Series(source_id)\n",
    "    author_ser = pd.Series(author)\n",
    "    title_ser = pd.Series(title)\n",
    "    des_ser = pd.Series(description)\n",
    "    url_ser = pd.Series(url)\n",
    "    content_ser = pd.Series(content)\n",
    "    date_ser = pd.Series(pub_date)\n",
    "\n",
    "    # Concatinate data of articles in one Series\n",
    "    News_data = pd.concat([source_id_ser, author_ser, title_ser, des_ser, url_ser, content_ser, date_ser], keys = ['source_id','author','title','description','url','content','pub_date'], axis = 1)\n",
    "    #print(News_data)\n",
    "\n",
    "    new_header = ['source_id', 'author', 'title', 'description', 'url', 'content', 'published_date']\n",
    "\n",
    "    # Write csv file with raw data of news article\n",
    "    News_data.to_csv(f'{file_name}', header = new_header)\n",
    "    \n",
    "    return News_data\n",
    "\n",
    "news = news_api('coronavirus', 100, 'cnn.com', '2020-01-27', './datasets/ref_news.csv')\n",
    "text = news_api('coronavirus', 100, 'theverge.com, foxnews.com', '2020-01-27', './datasets/test_text.csv')\n",
    "\n",
    "# Checking the data\n",
    "print(news.head(2))\n",
    "print(text.head(2))\n",
    "\n",
    "# Just saving some other methods\n",
    "#data = pd.read_csv(f'./datasets/liar_dataset/new_dataset.csv', delimiter=',')\n",
    "#for elements in range(len(data)):\n",
    "#    content = data.loc[elements, 'content']\n",
    "#    description = data.loc[elements, 'description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  0    1   2   3   4   5   \\\n",
      "0  Coronaviruses caused SARS, MERS, and the curre...  150  28   7   9   3   \n",
      "1  The coronavirus — which originated in Wuhan, C...  220  42   6  15   2   \n",
      "2    The State Department issued the travel advisory   41   7   1   4   1   \n",
      "3  Any foreign national who has traveled within C...  215  46  10  14   2   \n",
      "4  The World Health Organization declared a Publi...  218  43   7  17   3   \n",
      "\n",
      "          6         7   8    9   ...  16  17  18  19  20  21  22  23  24  25  \n",
      "0  14.000000  5.357143   0  0.0  ...   0   0   0   0   0   0  21   0   0   0  \n",
      "1  21.000000  5.214286   0  0.0  ...   0   2   0   0   0   0  32   0   0   0  \n",
      "2   7.000000  5.857143   0  0.0  ...   0   0   0   0   0   0   7   0   0   0  \n",
      "3  23.000000  4.673913   0  0.0  ...   0   1   0   0   0   0  37   0   1   0  \n",
      "4  14.333333  5.069767   0  0.0  ...   0   2   0   0   0   0  28   2   0   0  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "                                                  0    1   2   3   4   5   \\\n",
      "0  Apple is temporarily closing all of its stores...   78  15   2   5   2   \n",
      "1  Doctors have shared new details about the firs...  135  28   3  13   2   \n",
      "2  The Wuhan coronavirus outbreak is not a pandem...  147  25   7  11   2   \n",
      "3  As the coronavirus outbreak grows after killin...  150  28   3  11   2   \n",
      "4  Kenya Airways has suspended flights to and fro...  123  23   3  10   2   \n",
      "\n",
      "     6         7   8    9   ...  16  17  18  19  20  21  22  23  24  25  \n",
      "0  15.0  5.200000   0  0.0  ...   0   0   0   0   0   0  13   0   0   0  \n",
      "1  28.0  4.821429   0  0.0  ...   0   0   0   0   0   0  20   0   0   0  \n",
      "2  25.0  5.880000   0  0.0  ...   0   0   0   0   0   0  25   0   0   0  \n",
      "3  28.0  5.357143   0  0.0  ...   0   0   0   0   0   0  23   0   0   0  \n",
      "4  23.0  5.347826   0  0.0  ...   0   0   0   0   0   0  21   0   0   0  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This program is only for feature extraction from given data.\n",
    "\n",
    "At the beginning, the program grab the news contents (if content is null, use description. If both are\n",
    "null, drop that news.) from the given dataset and extract the features from the news contents.\n",
    "Then, it saves extracted data and features in a file.\n",
    "\n",
    "Features used in this program are:\n",
    "1. Number of Characters\n",
    "2. Number of Words\n",
    "3. Number of Verbs\n",
    "4. Number of Nouns\n",
    "5. Number of Sentence\n",
    "6. Average Number of Words per Sentence\n",
    "7. Average Number of Characters in Words\n",
    "8. Number of Question Marks\n",
    "9. Percentage of Subjective Verbs\n",
    "10. Percentage of Passive Voice\n",
    "11. Percentage of Positive Words\n",
    "12. Percentage of Negative Words\n",
    "13. Lexical Diversity: Unique Words or Terms\n",
    "14. Typographical Error Ration: Misspelled Words\n",
    "15. Causation Terms\n",
    "\n",
    "How to use: First, place positive-words.txt and negative-words.txt files in word_sentiment folder and put\n",
    "            .csv dataset in liar_dataset folder in datasets folder and put the name of the .csv file\n",
    "            in argument space of setup_data function. Next, use feature_extraction function with the result of\n",
    "            setup_data function to extract the features and save the data in the desired file.\n",
    "\n",
    "Date: Jan 28th, 2020\n",
    "Programmed by: Sasung Kim\n",
    "'''\n",
    "\n",
    "# Importing Required modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "#########################################################################################################\n",
    "# Create the lists of the words that used for feature extraction\n",
    "\n",
    "positive_list = list()\n",
    "negative_list = list()\n",
    "\n",
    "# Gather the positive and negative word lists from the text file\n",
    "with open('./word_sentiment/positive-words.txt') as p:\n",
    "    for line in p:\n",
    "        val = line.split()\n",
    "        positive_list.append(val)\n",
    "\n",
    "with open('./word_sentiment/negative-words.txt') as n:\n",
    "    for line in n:\n",
    "        val = line.split()\n",
    "        negative_list.append(val)\n",
    "\n",
    "subjective_list = list(['am ', 'are ', 'is ', 'was ', 'were ', 'be ', 'been '])\n",
    "causation_list = list(['led to ', 'because ', 'cause ', 'reason ', 'explanation ', 'so '])\n",
    "\n",
    "exclusive_list = list(['except ', 'else ', 'besides ', 'without ', 'exclude ', 'other than '])\n",
    "generalizing_list = list(['all ', 'none ', 'most ', 'many ', 'always ', 'everyone ','never ',\n",
    "                          'some ','usually ','few ','seldom ','generally ','general ','overall '])\n",
    "pronoun_1st_list = list(['I ','we '])\n",
    "pronoun_2nd3rd_list = list(['you','your','yours','he','she','it','him','her','his','her','its','hers','They','them','theirs','their'])\n",
    "\n",
    "SPronoun_list = list(['I', 'mine','my','me'])\n",
    "GPronoun_list = list(['we','ours','our','us'])\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "# From the dataset, isolate the required data (statement and label in this program) and return as pandas Series\n",
    "# If the user needs the required data as a file, uncomment def with new_file as argument and comment the other one\n",
    "\n",
    "#def setup_data(file_name, new_file):\n",
    "def setup_data(file_name):\n",
    "    data = pd.read_csv(f'{file_name}', delimiter=',')\n",
    "\n",
    "    # Check for the raw data from original dataset\n",
    "    # print (list(data.columns.values))\n",
    "    # print(data.tail(1))\n",
    "\n",
    "    news_contents = []\n",
    "    \n",
    "    for elements in range(len(data)):\n",
    "        description = data.loc[elements, 'description']\n",
    "        \n",
    "        # uncomment and indent the news_contents.append(description) line if both contents and descriptions are used instead of  description only\n",
    "        \n",
    "        #content = data.loc[elements, 'content']\n",
    "        #if not pd.isna(content) and content is \"Chat with us in Facebook Messenger. Find out what's happening in the world as it unfolds.\":\n",
    "        #    news_contents.append(content)\n",
    "        #else:\n",
    "        news_contents.append(description)\n",
    "    \n",
    "    # drop the null values (if both content and description are null)\n",
    "    news_contents = [x for x in news_contents if x == x]\n",
    "    \n",
    "    # Check if the required data are correctly input\n",
    "    #print(news_contents)\n",
    "    \n",
    "    return news_contents\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "# Not required for right now, but may used in later for more efficient tockenization and feature extraction\n",
    "# vectorizer = CountVectorizer()\n",
    "# train_count = vectorizer.fit_transform(data[\"statement\"].values)\n",
    "\n",
    "# Check for vectorizer\n",
    "# print(train_count.shape)\n",
    "# print(vectorizer.vocabulary_)\n",
    "# print(vectorizer.get_feature_names())\n",
    "\n",
    "# Input: String ('str')\n",
    "# Description: Tockenize and tag input with nltk universal tag\n",
    "# Return: tags of each tockens - String ('tagged')\n",
    "# Tockenize and tag input String with nltk universal tag and return the tags for each words in the String\n",
    "\n",
    "def tagging_univ(str):\n",
    "    text = nltk.word_tokenize(str)\n",
    "    tagged = nltk.pos_tag(text, tagset='universal')\n",
    "    return tagged\n",
    "\n",
    "# Input: String('str')\n",
    "# Description: Tockenize and tag input with nltk non-universal tag\n",
    "# Return: tags of each tockens - String ('tagged')\n",
    "\n",
    "def tagging_nuniv(str):\n",
    "    text = nltk.word_tokenize(str)\n",
    "    tagged = nltk.pos_tag(text)\n",
    "    return tagged\n",
    "\n",
    "# Input: String('str')\n",
    "# Description: Count the number of characters in input\n",
    "# Return: Character count - int ('count')\n",
    "\n",
    "def count_char(str):\n",
    "    no_space = str.replace(\" \", \"\")\n",
    "    count = len(no_space)\n",
    "    return count\n",
    "\n",
    "# Input: String('str')\n",
    "# Description: Count the number of words in input\n",
    "# Return: Word count - int ('count')\n",
    "\n",
    "def count_word(str):\n",
    "    count = len(str.split())\n",
    "    return count\n",
    "\n",
    "# Input: String ('tags'), int(verb_count)\n",
    "# Description: Count the number of verbs in input\n",
    "# Return: Verb count - int ('verb_count')\n",
    "\n",
    "def count_verb(tags, verb_count):\n",
    "    if (tags == 'VERB'):\n",
    "        verb_count += 1\n",
    "    return verb_count\n",
    "\n",
    "# Input: String('tags'), int('noun_count')\n",
    "# Description: Count the number of nouns in input\n",
    "# Return: Noun count - int ('noun_count')\n",
    "\n",
    "def count_noun(tags, noun_count):\n",
    "    if (tags == 'NOUN'):\n",
    "        noun_count += 1\n",
    "    return noun_count\n",
    "\n",
    "# Input: String ('str')\n",
    "# Description: Count the number of sentences by counting number of period(.)\n",
    "# Return: Sentence count - int ('sentence')\n",
    "\n",
    "def count_sent(str):\n",
    "    sentence = len(str.split('.'))\n",
    "    return sentence\n",
    "\n",
    "# Input: String ('states')\n",
    "# Description: Count the number of characters in each word in input and average the number of characters per word\n",
    "# Return: Average number of characters: float ('avg')\n",
    "\n",
    "def count_char_per_word(states):\n",
    "    word = []\n",
    "    word.append(states.split())\n",
    "    char_per_word = list()\n",
    "    for elements in word:\n",
    "        for char in elements:\n",
    "            c_in_w_count = len(char)\n",
    "            char_per_word.append(c_in_w_count)\n",
    "    # char_per_word_list.append()\n",
    "    avg = sum(char_per_word) / len(char_per_word)\n",
    "    char_per_word.clear()\n",
    "    return avg\n",
    "\n",
    "# Input: String ('tagged'), list ('list'), int ('int')\n",
    "# Description: Count the number of common words between input String and input list\n",
    "# Return: Count of common words - int ('int')\n",
    "# Count and return the number of common words between input String ('tagged') and input list\n",
    "\n",
    "def check_common(tagged, list, int):\n",
    "    for elements in list:\n",
    "        if tagged in elements:\n",
    "            int += 1\n",
    "            return int\n",
    "    return int\n",
    "\n",
    "# Input: list ('subjective_list'), int ('verb_count'), String ('states')\n",
    "# Description: Count the number of subjective words and avarage it from number of verbs in input\n",
    "# Return: Average of subjective words - float ('percent_sub')\n",
    "\n",
    "def count_sub(subjective_list, verb_count, states):\n",
    "    percent_sub = 0\n",
    "    for ele in subjective_list:\n",
    "        sub_count = states.count(ele)\n",
    "    if (verb_count > 0):\n",
    "        percent_sub = sub_count / verb_count * 100\n",
    "    return percent_sub\n",
    "\n",
    "# Input: String ('word'), String ('tag'), list ('subjective_list'), int ('sent_count')\n",
    "# Description: Count the number of passive voice and avarge it from number of sentence (sent_count)\n",
    "# Return: Average of passive voiced sentences - float ('result')\n",
    "\n",
    "def count_passive(word, tag, subjective_list, sent_count):\n",
    "    percent_sub = 0\n",
    "    counter = 0\n",
    "    for ele in subjective_list:\n",
    "        if (word.count(ele) > 0 and tag == \"VBN\"):\n",
    "            counter += 1\n",
    "    result = counter / sent_count * 100\n",
    "    return result\n",
    "\n",
    "# Input: String ('states')\n",
    "# Description: Count the words that introduced only once in input\n",
    "# Return: Count of unique words - int ('unique_count')\n",
    "\n",
    "def count_unique(states):\n",
    "    words = states.split(' ')\n",
    "    c = Counter(words)\n",
    "    unique = [w for w in words if c[w] == 1]\n",
    "    unique_counter = len(unique)\n",
    "    return unique_counter\n",
    "\n",
    "# Input: String ('tag'), int ('typo_count')\n",
    "# Description: Count the number of words with nltk universal tag is \"X\" (misspelled)\n",
    "# Return: Count of misspelled words - int ('typo_count')\n",
    "\n",
    "def count_typo(tag, typo_count):\n",
    "    if (tag == \"X\"):\n",
    "        typo_count += 1\n",
    "    return typo_count\n",
    "\n",
    "# Input: list ('causation_list'), String ('states')\n",
    "# Description: Count the number of common words in list input and String input\n",
    "# Return: Count of common words - int ('cause_count')\n",
    "\n",
    "def count_cause(causation_list, states):\n",
    "    for ele in causation_list:\n",
    "        cause_count = states.count(ele)\n",
    "    return cause_count\n",
    "\n",
    "# Calvin\n",
    "\n",
    "def count_num(tags, num_count):\n",
    "    if(tags == 'NUM'):\n",
    "        num_count += 1\n",
    "    return num_count\n",
    "\n",
    "def avg_char(states):\n",
    "    word.append(states.split())\n",
    "    char_per_word = list()\n",
    "    for elements in word:\n",
    "        for char in elements:\n",
    "            c_in_w_count = len(char)\n",
    "            char_per_word.append(c_in_w_count)\n",
    "    word.clear()\n",
    "    avg = sum(char_per_word)/len(char_per_word)\n",
    "    char_per_word.clear()\n",
    "    return avg\n",
    "\n",
    "def avg_punc(tags, punc_count):\n",
    "    if(tags == '.'):\n",
    "        punc_count += 1\n",
    "    return punc_count\n",
    "\n",
    "def count_gene(generalizing_list, states):\n",
    "    for ele in generalizing_list:\n",
    "        gene_count = states.count(ele)\n",
    "    return gene_count\n",
    "\n",
    "def count_exclu(exclusive_list, states):\n",
    "    for ele in exclusive_list:\n",
    "        exclu_count = states.count(ele)\n",
    "    return exclu_count\n",
    "\n",
    "def count_pron_2nd3rd(pronoun_1st_list, states):\n",
    "    for ele in pronoun_1st_list:\n",
    "        pron_2nd3rd_count = states.count(ele)\n",
    "    return pron_2nd3rd_count\n",
    "\n",
    "def count_pron_1st(pronoun_1st_list, states):\n",
    "    for ele in pronoun_1st_list:\n",
    "        pron_1st_count = states.count(ele)\n",
    "    return pron_1st_count\n",
    "\n",
    "# Andrew\n",
    "\n",
    "#self pronoun 1st person\n",
    "def count_pronself(SPronoun_list, states):\n",
    "    for ele in SPronoun_list:\n",
    "        SelfP = states.count(ele)\n",
    "        return SelfP\n",
    "    \n",
    "#Group ref 1st person\n",
    "def count_prongroup(GPronoun_list, states):\n",
    "    for ele in GPronoun_list:\n",
    "        GP = states.count(ele)\n",
    "        return GP\n",
    "    \n",
    "# 2nd and 3rd person pronouns\n",
    "def count_pronother(Pronoun_list, states):\n",
    "     for ele in Pronoun_list:\n",
    "         othernouns = states.count(ele)\n",
    "         return othernouns\n",
    "\n",
    "\n",
    "# Input: panda Series ('data'), String ('save_file_name')\n",
    "# Description: First, this function creates lists for each features and extract the features using statements in dataset and\n",
    "#              above functions. Next, it creates a large pandas Series that consist of news contents, labels and extracted\n",
    "#              features. Finally, it save the final pandas Series in a file to make easier to examine the result (do not need\n",
    "#              to rerun the program or change the code to check raw data)\n",
    "# Return: Pandas Series consist of news contents, labels and extracted features count - pandas Series ('new')\n",
    "\n",
    "def feature_extract(data, save_file_name):\n",
    "    # define the news contents and labels from the dataset\n",
    "    state = data\n",
    "\n",
    "    # create lists for storing the counters\n",
    "    char_count_list = list()\n",
    "    word_count_list = list()\n",
    "    verb_count_list = list()\n",
    "    noun_count_list = list()\n",
    "    sent_count_list = list()\n",
    "    words_per_sent_list = list()\n",
    "    char_per_word_list = list()\n",
    "    quest_count_list = list()\n",
    "    sub_count_list = list()\n",
    "    pass_count_list = list()\n",
    "    pos_count_list = list()\n",
    "    neg_count_list = list()\n",
    "    unique_count_list = list()\n",
    "    typo_count_list = list()\n",
    "    cause_count_list = list()\n",
    "    \n",
    "    # Calvin\n",
    "    gene_count_list = list()\n",
    "    num_count_list = list()\n",
    "    pron_1st_count_list = list()\n",
    "    pron_2nd3rd_count_list = list()\n",
    "    exclu_count_list = list()\n",
    "    \n",
    "    # Andrew\n",
    "    exclam_list = list()\n",
    "    lex_list = list()\n",
    "    singlulars_list = list()\n",
    "    group_list = list()\n",
    "    other_list = list()\n",
    "\n",
    "\n",
    "    word = list()\n",
    "\n",
    "    #print(type(state))\n",
    "    # loop for checking each new contents in dataset\n",
    "    for states in state:\n",
    "\n",
    "        #print(states)\n",
    "        # reset the counters for each news contents\n",
    "        w_in_s_count = 0\n",
    "        c_in_w_count = 0\n",
    "        verb_count = 0\n",
    "        noun_count = 0\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        percent_pos = 0\n",
    "        percent_neg = 0\n",
    "        unique_count = 0\n",
    "        sent_counts = 0\n",
    "        typo_count = 0\n",
    "        num= 0\n",
    "        \n",
    "\n",
    "        # Tockenization and tagging with nltk universal and non-universal tag systems (tagged = universal, tagged_nu = non-universal)\n",
    "        tagged = tagging_univ(states)\n",
    "        tagged_nu = tagging_nuniv(states)\n",
    "\n",
    "        # Check the tags of each news contents\n",
    "        # print(tagged)\n",
    "        # print(tagged_nu)\n",
    "\n",
    "        # Extract the features and append the results in the list. Commented lines with print() functions are for testing\n",
    "\n",
    "        # 1. Number of Characters\n",
    "        char_count = count_char(states)\n",
    "        char_count_list.append(char_count)\n",
    "\n",
    "        # 2. Nubmer of Words\n",
    "        word_count = count_word(states)\n",
    "        word_count_list.append(word_count)\n",
    "\n",
    "        # 3. Number of Verbs\n",
    "        for tag in tagged:\n",
    "            verb_count = count_verb(tag[1], verb_count)\n",
    "        verb_count_list.append(verb_count)\n",
    "\n",
    "        # 4. Number of Nouns\n",
    "        for tag in tagged:\n",
    "            noun_count = count_noun(tag[1], noun_count)\n",
    "        noun_count_list.append(noun_count)\n",
    "        # print(noun_count)\n",
    "\n",
    "        # 5. Number of Sentence\n",
    "        sent_count = count_sent(states)\n",
    "        sent_count_list.append(sent_count)\n",
    "        # print(sent_count)\n",
    "\n",
    "        # 6. Average number of words per sentence\n",
    "        sent = [len(l.split()) for l in re.split(r'[?!.]', states) if l.strip()]\n",
    "        w_in_s_count = (sum(sent) / len(sent))\n",
    "        words_per_sent_list.append(w_in_s_count)\n",
    "        # print(w_in_s_count)\n",
    "\n",
    "        # 7. Average number of characters per word\n",
    "        c_in_w_count = count_char_per_word(states)\n",
    "        char_per_word_list.append(c_in_w_count)\n",
    "        # print(c_in_w_count)\n",
    "\n",
    "        # 8. Number of question marks\n",
    "        quest_count = states.count(\"?\")\n",
    "        quest_count_list.append(quest_count)\n",
    "\n",
    "        # 9. Percentage of subjective verbs - am/are/is/etc\n",
    "        percent_sub = count_sub(subjective_list, verb_count, states)\n",
    "        sub_count_list.append(percent_sub)\n",
    "\n",
    "        # 10. Percentage of passive voice - am/are/is && past participate\n",
    "        for tag in tagged_nu:\n",
    "            passive_percent = count_passive(tag[0], tag[1], subjective_list, sent_count)\n",
    "        pass_count_list.append(passive_percent)\n",
    "\n",
    "        # 11. Percentage of positive words\n",
    "        for tag in tagged:\n",
    "            pos_count = check_common(tag[0], positive_list, pos_count)\n",
    "            percent_pos = pos_count / word_count * 100\n",
    "        pos_count_list.append(percent_pos)\n",
    "\n",
    "        # 12. Percentage of negative words\n",
    "        for tag in tagged:\n",
    "            neg_count = check_common(tag[0], negative_list, neg_count)\n",
    "            percent_neg = neg_count / word_count * 100\n",
    "        neg_count_list.append(percent_neg)\n",
    "\n",
    "        # 13. Lexical diversity: unique words or terms\n",
    "        unique_count = count_unique(states)\n",
    "        unique_count_list.append(unique_count)\n",
    "\n",
    "        # 14. Typographical error ratio: misspelled words\n",
    "        for tag in tagged:\n",
    "            typo_count = count_typo(tag[1], typo_count)\n",
    "        typo_count_list.append(typo_count)\n",
    "\n",
    "        # 15. Causation terms\n",
    "        cause_count = count_cause(causation_list, states)\n",
    "        cause_count_list.append(cause_count)\n",
    "\n",
    "        # 16. Percentage of generalizing terms\n",
    "        gene_count = count_gene(generalizing_list, states)\n",
    "        gene_count_list.append(gene_count)\n",
    "        \n",
    "        # 17. Percentage of numbers and quantifiers\n",
    "        for tag in tagged:\n",
    "            num = count_num(tag[1], num)\n",
    "        num_count_list.append(num)\n",
    "\n",
    "        # 18. 1st person pronouns\n",
    "        pron_1st_count = count_pron_1st(pronoun_1st_list, states)\n",
    "        pron_1st_count_list.append(pron_1st_count)\n",
    "   \n",
    "        # 19. 2nd and 3rd person pronouns\n",
    "        pron_count = count_pron_2nd3rd(pronoun_2nd3rd_list, states)\n",
    "        pron_2nd3rd_count_list.append(pron_count)\n",
    "\n",
    "        # 20. Exclusive terms\n",
    "        exclu_count = count_exclu(exclusive_list, states)\n",
    "        exclu_count_list.append(exclu_count)\n",
    "        \n",
    "        # 21. # of exclamation marks\n",
    "        exclam_count = states.count(\"!\")\n",
    "        exclam_list.append(exclam_count)\n",
    "        \n",
    "        # 22. lexical \n",
    "        unique_count = count_unique(states)\n",
    "        lex_list.append(unique_count)\n",
    "        \n",
    "        # 23. singlular pronouns (1st person)\n",
    "        SelfP = count_pronself(SPronoun_list, states)\n",
    "        singlulars_list.append(SelfP)\n",
    "        \n",
    "        # 24. Group ref pronouns (1st person)\n",
    "        group = count_prongroup(GPronoun_list, states)\n",
    "        group_list.append(group)\n",
    "        \n",
    "        # 25. 2nd 3rd pronouns\n",
    "        other = count_pronother(pronoun_2nd3rd_list, states)\n",
    "        other_list.append(other)\n",
    "        \n",
    "    # Convert lists into pandas Series\n",
    "    statement = pd.Series(state)\n",
    "    first = pd.Series(char_count_list)\n",
    "    second = pd.Series(word_count_list)\n",
    "    third = pd.Series(verb_count_list)\n",
    "    fourth = pd.Series(noun_count_list)\n",
    "    fifth = pd.Series(sent_count_list)\n",
    "    sixth = pd.Series(words_per_sent_list)\n",
    "    seventh = pd.Series(char_per_word_list)\n",
    "    eighth = pd.Series(quest_count_list)\n",
    "    nineth = pd.Series(sub_count_list)\n",
    "    tenth = pd.Series(pass_count_list)\n",
    "    eleventh = pd.Series(pos_count_list)\n",
    "    twelveth = pd.Series(neg_count_list)\n",
    "    thirteenth = pd.Series(unique_count_list)\n",
    "    fourteenth = pd.Series(typo_count_list)\n",
    "    fifteenth = pd.Series(cause_count_list)\n",
    "    \n",
    "    \n",
    "    sixteenth = pd.Series(gene_count_list)\n",
    "    seventeenth = pd.Series(num_count_list)\n",
    "    eighteenth = pd.Series(pron_1st_count_list)\n",
    "    nineteenth = pd.Series(pron_2nd3rd_count_list)\n",
    "    twentieth = pd.Series(exclu_count_list)\n",
    "    twenty_first = pd.Series(exclam_list)\n",
    "    twenty_second = pd.Series(lex_list)\n",
    "    twenty_third = pd.Series(singlulars_list)\n",
    "    twenty_fourth = pd.Series(group_list)\n",
    "    twenty_fifth = pd.Series(other_list)\n",
    "\n",
    "    # Concatenate all feature Series, news contents and labels into one pandas Series\n",
    "    new = pd.concat([statement, first, second, third, fourth, fifth, sixth, seventh, eighth, nineth, tenth,\n",
    "                     eleventh, twelveth, thirteenth, fourteenth, fifteenth, sixteenth, seventeenth, eighteenth,\n",
    "                    nineteenth, twentieth, twenty_first, twenty_second, twenty_third, twenty_fourth, twenty_fifth], axis=1)\n",
    "\n",
    "    # Test for the final Series\n",
    "    # print(new.head(3))\n",
    "\n",
    "    \n",
    "    # 16. Percentage of generalizing terms\n",
    " # 17. Percentage of numbers and quantifiers\n",
    "# 18. 1st person pronouns\n",
    "# 19. 2nd and 3rd person pronouns\n",
    "# 20. Exclusive terms\n",
    "# 21. # of exclamation marks\n",
    "# 22. lexical\n",
    " # 23. singlular pronouns (1st person)\n",
    "# 24. Group ref pronouns (1st person)\n",
    "# 25. 2nd 3rd pronouns\n",
    "    \n",
    "    # Name the header of the data\n",
    "    new_header = ['Statement', '# of Characters', '# of Words', '# of Verbs', '# of Noun', '# of Sentence',\n",
    "                  'Average # of Words per Sentence', 'Average # of Characters per Words', '# of Question Marks',\n",
    "                  '% of Subjective Verbs', '% of Passive Voice', '% of Positive Words', '% of Negative Words',\n",
    "                  '# of Unique Wrods/Terms', '# of Misspelled Words', '# of Causation Terms', \"% of generalizing terms\",\n",
    "                 \"% of # and quantifiers\", \"1st person pronouns\", \"2nd and 3rd person pronouns\", \"Exclusive term\",\n",
    "                 \"# of exclamation marks\", \"Lexical\", \"Singular pronouns(1st person)\", \"Group ref pronouns(1st person)\",\n",
    "                 \"2nd 3rd pronouns\"]\n",
    "\n",
    "    # Save the data in a file\n",
    "\n",
    "    #with open(save_file_name, 'w') as f:\n",
    "    #    writer = csv.DictWriter(f, fieldnames=new_header)\n",
    "    #    writer.writeheader()\n",
    "\n",
    "    new.to_csv(save_file_name, header=new_header)\n",
    "    \n",
    "    return new\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# Data Extraction\n",
    "# From the dataset file, gather the news contents and labels only\n",
    "\n",
    "text_data = setup_data('./datasets/test_text.csv')\n",
    "news_data = setup_data('./datasets/ref_news.csv')\n",
    "\n",
    "# Check for shape and contents of isolated data\n",
    "# print(train_data.shape)\n",
    "# print(test_data.shape)\n",
    "# print (valid_data.shape)\n",
    "# print (train_data.head(5))\n",
    "# print (test_data.head(5))\n",
    "# print (valid_data.head(5))\n",
    "\n",
    "# Feature Extraction\n",
    "\n",
    "text_data_FE = feature_extract(text_data, './text_FE.csv')\n",
    "news_data_FE = feature_extract(news_data, './news_FE.csv')\n",
    "\n",
    "# Checking data\n",
    "\n",
    "print(text_data_FE.head(5))\n",
    "print(news_data_FE.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                TEXT  \\\n",
      "0  Coronaviruses caused SARS, MERS, and the curre...   \n",
      "1  The coronavirus — which originated in Wuhan, C...   \n",
      "2    The State Department issued the travel advisory   \n",
      "3  Any foreign national who has traveled within C...   \n",
      "4  The World Health Organization declared a Publi...   \n",
      "5  The Centers for Disease Control and Prevention...   \n",
      "6  Coronavirus outbreak: What to do if you think ...   \n",
      "7  Foxconn has stopped “almost all” of its produc...   \n",
      "8  American Airlines announced that it’s suspendi...   \n",
      "9  Mark Zuckerberg says people need to understand...   \n",
      "\n",
      "                                           REFERENCE  # of Characters - sub  \\\n",
      "0  Apple is temporarily closing all of its stores...                  -72.0   \n",
      "1  Doctors have shared new details about the firs...                  -85.0   \n",
      "2  The Wuhan coronavirus outbreak is not a pandem...                  106.0   \n",
      "3  As the coronavirus outbreak grows after killin...                  -65.0   \n",
      "4  Kenya Airways has suspended flights to and fro...                  -95.0   \n",
      "5  The US has begun implementing new rules around...                  -18.0   \n",
      "6  The number of confirmed cases of the Wuhan cor...                  -57.0   \n",
      "7  The Wuhan coronavirus leaves the streets of Wu...                  -34.0   \n",
      "8  Streets are empty and markets are closed in Ch...                  -73.0   \n",
      "9  Macao, known as China's gambling haven, has be...                   67.0   \n",
      "\n",
      "   # of Words - sub  # of Verbs - sub  # of Noun - sub  # of Sentence - sub  \\\n",
      "0             -13.0              -5.0             -4.0                 -1.0   \n",
      "1             -14.0              -3.0             -2.0                  0.0   \n",
      "2              18.0               6.0              7.0                  1.0   \n",
      "3             -18.0              -7.0             -3.0                  0.0   \n",
      "4             -20.0              -4.0             -7.0                 -1.0   \n",
      "5              -1.0               5.0             -2.0                 -2.0   \n",
      "6             -15.0              -2.0            -12.0                  1.0   \n",
      "7             -12.0               0.0             -4.0                  0.0   \n",
      "8             -12.0              -2.0             -7.0                  1.0   \n",
      "9              11.0               2.0              8.0                  1.0   \n",
      "\n",
      "   Average # of Words per Sentence - sub  \\\n",
      "0                                   1.00   \n",
      "1                                   7.00   \n",
      "2                                  18.00   \n",
      "3                                   5.00   \n",
      "4                                   8.67   \n",
      "5                                  21.00   \n",
      "6                                 -15.00   \n",
      "7                                  -6.00   \n",
      "8                                  -6.00   \n",
      "9                                   5.50   \n",
      "\n",
      "   Average # of Characters per Words - sub  # of Question Marks - sub  ...  \\\n",
      "0                                    -0.16                        0.0  ...   \n",
      "1                                    -0.39                        0.0  ...   \n",
      "2                                     0.02                        0.0  ...   \n",
      "3                                     0.68                        0.0  ...   \n",
      "4                                     0.28                        0.0  ...   \n",
      "5                                    -0.37                        0.0  ...   \n",
      "6                                     0.68                        0.0  ...   \n",
      "7                                     0.84                        0.0  ...   \n",
      "8                                    -0.30                        0.0  ...   \n",
      "9                                     0.44                        0.0  ...   \n",
      "\n",
      "   % of generalizing terms - avg sub  % of # and quantifiers - avg sub  \\\n",
      "0                                0.0                             -0.47   \n",
      "1                                0.0                              1.53   \n",
      "2                                0.0                             -0.47   \n",
      "3                                0.0                              0.53   \n",
      "4                                0.0                              1.53   \n",
      "5                                0.0                             -0.47   \n",
      "6                                0.0                              0.53   \n",
      "7                                0.0                              1.53   \n",
      "8                                0.0                              1.53   \n",
      "9                                0.0                             -0.47   \n",
      "\n",
      "   1st person pronouns - avg sub  2nd and 3rd person pronouns - avg sub  \\\n",
      "0                          -0.01                                  -0.03   \n",
      "1                          -0.01                                  -0.03   \n",
      "2                          -0.01                                  -0.03   \n",
      "3                          -0.01                                  -0.03   \n",
      "4                          -0.01                                  -0.03   \n",
      "5                          -0.01                                  -0.03   \n",
      "6                          -0.01                                  -0.03   \n",
      "7                          -0.01                                  -0.03   \n",
      "8                          -0.01                                  -0.03   \n",
      "9                          -0.01                                  -0.03   \n",
      "\n",
      "   Exclusive term - avg sub  # of exclamation marks - avg sub  \\\n",
      "0                       0.0                               0.0   \n",
      "1                       0.0                               0.0   \n",
      "2                       0.0                               0.0   \n",
      "3                       0.0                               0.0   \n",
      "4                       0.0                               0.0   \n",
      "5                       0.0                               0.0   \n",
      "6                       0.0                               0.0   \n",
      "7                       0.0                               0.0   \n",
      "8                       0.0                               0.0   \n",
      "9                       0.0                               0.0   \n",
      "\n",
      "   Lexical - avg sub  Singular pronouns(1st person) - avg sub  \\\n",
      "0              -6.26                                    -0.17   \n",
      "1               4.74                                    -0.17   \n",
      "2             -20.26                                    -0.17   \n",
      "3               9.74                                    -0.17   \n",
      "4               0.74                                     1.83   \n",
      "5              -4.26                                    -0.17   \n",
      "6              11.74                                    -0.17   \n",
      "7               0.74                                    -0.17   \n",
      "8               1.74                                     0.83   \n",
      "9              -8.26                                    -0.17   \n",
      "\n",
      "   Group ref pronouns(1st person) - avg sub  2nd 3rd pronouns - avg sub  \n",
      "0                                     -0.11                         0.0  \n",
      "1                                     -0.11                         0.0  \n",
      "2                                     -0.11                         0.0  \n",
      "3                                      0.89                         0.0  \n",
      "4                                     -0.11                         0.0  \n",
      "5                                     -0.11                         0.0  \n",
      "6                                     -0.11                         2.0  \n",
      "7                                     -0.11                         0.0  \n",
      "8                                      0.89                         0.0  \n",
      "9                                     -0.11                         0.0  \n",
      "\n",
      "[10 rows x 52 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This program extract the multi-sourced features from two datasets (one is unreliable dataset that text that is evaluated, one for reliable dataset as reference news of evaluation).\n",
    "\n",
    "Restrictions: Two provided datasets must have the same number of data and have same feature and name. For better use, it is recommended to use the news/texts that have same topics.\n",
    "\n",
    "How to use: Place the datasets that consists of statements and feature values in a folder and put the name or path of the datasets in news1 and news2 arguments of multi_source_FE() definition.\n",
    "            news1 should be the dataset of referencing articles, and news2 should be the dataset of testing texts\n",
    "\n",
    "Date: Feb 2, 2020\n",
    "Programmed by: Sasung Kim\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Import Required Modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from statistics import mean\n",
    "\n",
    "# Import Required Dictionaries for Feature Extraction\n",
    "\n",
    "positive_list = list()\n",
    "negative_list = list()\n",
    "\n",
    "# Gather the positive and negative word lists from the text file\n",
    "with open('./word_sentiment/positive-words.txt') as p:\n",
    "    for line in p:\n",
    "        val = line.split()\n",
    "        positive_list.append(val)\n",
    "\n",
    "with open('./word_sentiment/negative-words.txt') as n:\n",
    "    for line in n:\n",
    "        val = line.split()\n",
    "        negative_list.append(val)\n",
    "\n",
    "subjective_list = list(['am ', 'are ', 'is ', 'was ', 'were ', 'be ', 'been '])\n",
    "causation_list = list(['led to ', 'because ', 'cause ', 'reason ', 'explanation ', 'so '])\n",
    "\n",
    "# Define the definitions for extracting features\n",
    "\n",
    "# Input: fileName (String)\n",
    "# Description: Read the .csv file and return the data as list\n",
    "# Return: data (list)\n",
    "\n",
    "def data_read(fileName):\n",
    "    with open (fileName, encoding = 'utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        first_row = next(reader, None)\n",
    "        data = list(reader)\n",
    "        return data, first_row\n",
    "\n",
    "def header_extension(data_list, startColumn, extension, new_header):\n",
    "    for i in range(startColumn, len(data_list)):\n",
    "        new_header.append(f'{data_list[i]} - {extension}')\n",
    "    return new_header\n",
    "    \n",
    "    \n",
    "# Subtract each features for each data in the same row (one-to-one) and save the data in total_list. List contains\n",
    "# the extracted feature values related to texts. Ex. total_list[0] = all feature values of first statement\n",
    "# To use absolute value, uncomment first feature and comment second feature.\n",
    "\n",
    "def one_to_one_dif(text, ref, startFColumn, feature_num):\n",
    "    feature_list = []\n",
    "    for i in range (startFColumn, feature_num):\n",
    "        try:\n",
    "            #feature = (abs(float(data2[i]) - float(data1[i])))\n",
    "            feature = (float(text[i]) - float(ref[i]))\n",
    "        except:\n",
    "            feature = (float(text[i]))\n",
    "                \n",
    "        feature_list.append(round(feature, 2))\n",
    "    return feature_list\n",
    "    \n",
    "    \n",
    "def one_to_many_dif(ref, text, feature_num, feature_pos):\n",
    "    avg_list = [];\n",
    "    for i in range(len(ref)):\n",
    "        feature_val = float(text[feature_pos]) - float(ref[i][feature_pos])\n",
    "        avg_list.append(feature_val)\n",
    "    avg = round(mean(avg_list), 2)\n",
    "    return avg\n",
    "    \n",
    "    \n",
    "# Input: news1 (String), news2 (String), outFile (String), startFColumn (integer)\n",
    "# Description: Read the data from news1 (reliable) and news2 (unreliable), and put the data in two lists. Then, subtract the news1 values in each column from news2 values.\n",
    "#              The subtraction starts from startFColumn until the end of the .csv file. Next, the results will be saved in the file with given name or path from user (outFile)\n",
    "#\n",
    "# Return: New DataFrame ('MS_features')\n",
    "\n",
    "def multi_source_FE(news1, news2, outFile, startFColumn):\n",
    "    \n",
    "    # Reading data from .csv files\n",
    "    data1 = data_read(f'./{news1}')\n",
    "    data2 = data_read(f'./{news2}')\n",
    "    \n",
    "    # Saving original headers in header\n",
    "    header = data1[1]\n",
    "    feature_num = len(header)\n",
    "    \n",
    "    # Create header list with modified header names. Can be changed later with different method of multi-sourcing by modifying '- sub'.\n",
    "    header_edit = ['text', 'news']\n",
    "    header_edit = header_extension(header, 2, 'sub', header_edit)\n",
    "    header_edit = header_extension(header, 2, 'avg sub', header_edit)\n",
    "    \n",
    "    #for i in range(startFColumn, len(data1[1])):\n",
    "     #   header.append(f'{data1[1][i]} - sub')\n",
    "      #  header.append(f'{data1[1][i]} - avg sub')\n",
    "    \n",
    "    # Assign data of the testing text and referencing text in data1 and data2\n",
    "    data1 = data1[0]\n",
    "    data2 = data2[0]\n",
    "    \n",
    "    # Create local lists, dataframe that are used later in this definition.\n",
    "    text = []\n",
    "    ref = []\n",
    "    total_list = []\n",
    "    new_total_list = []\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Loop through the data for subtraction\n",
    "    for i in range(len(data2)):\n",
    "        \n",
    "        # Statement of each data (text - text trying to verify, ref - reliable articles to reference for text comparison)\n",
    "        try:\n",
    "            ref.append(data1[i][1])\n",
    "            text.append(data2[i][1])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #print(one_to_one_dif(data1[i], data2[i], startFColumn, feature_num))\n",
    "        try:\n",
    "            total_list.append(one_to_one_dif(data1[i], data2[i], startFColumn, feature_num))\n",
    "            for j in range(startFColumn, feature_num):\n",
    "                total_list[i].append(one_to_many_dif(data1, data2[i], feature_num, j))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    # Reshape the lists in terms of features. Ex. new_total_list[0] = all values of first feature\n",
    "    for i in range(len(total_list)):\n",
    "        new_list = []\n",
    "        for element in (total_list):\n",
    "            try:\n",
    "                new_list.append(element[i])\n",
    "            except:\n",
    "                pass\n",
    "        new_total_list.append(new_list)\n",
    "       \n",
    "    # Creating dataframe with multi-sourced features\n",
    "    df['TEXT'] = text\n",
    "    df['REFERENCE'] = ref\n",
    "    for i in range(2, len(header_edit)):\n",
    "        df[header_edit[i]] = new_total_list[i-2]\n",
    "    \n",
    "    # Saving data into file\n",
    "    df.to_csv(f'./{outFile}')\n",
    "    return df\n",
    "\n",
    "# Initiate the code and check for the dataframe\n",
    "MSFE = multi_source_FE('news_FE.csv', 'text_FE.csv', 'OtoO_FE.csv', 2)\n",
    "print(MSFE.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This program extract the multi-sourced features from two datasets (one is unreliable dataset that text that is evaluated, one for reliable dataset as reference news of evaluation).\n",
    "\n",
    "Restrictions: Two provided datasets must have the same number of data and have same feature and name. For better use, it is recommended to use the news/texts that have same topics.\n",
    "\n",
    "How to use: Place the datasets that consists of statements and feature values in a folder and put the name or path of the datasets in news1 and news2 arguments of multi_source_FE() definition.\n",
    "            news1 should be the dataset of referencing articles, and news2 should be the dataset of testing texts\n",
    "\n",
    "Date: Feb 2, 2020\n",
    "Programmed by: Sasung Kim\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Import Required Modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from statistics import mean\n",
    "\n",
    "# Import Required Dictionaries for Feature Extraction\n",
    "\n",
    "positive_list = list()\n",
    "negative_list = list()\n",
    "\n",
    "# Gather the positive and negative word lists from the text file\n",
    "with open('./word_sentiment/positive-words.txt') as p:\n",
    "    for line in p:\n",
    "        val = line.split()\n",
    "        positive_list.append(val)\n",
    "\n",
    "with open('./word_sentiment/negative-words.txt') as n:\n",
    "    for line in n:\n",
    "        val = line.split()\n",
    "        negative_list.append(val)\n",
    "\n",
    "subjective_list = list(['am ', 'are ', 'is ', 'was ', 'were ', 'be ', 'been '])\n",
    "causation_list = list(['led to ', 'because ', 'cause ', 'reason ', 'explanation ', 'so '])\n",
    "\n",
    "# Define the definitions for extracting features\n",
    "\n",
    "# Input: fileName (String)\n",
    "# Description: Read the .csv file and return the data as list\n",
    "# Return: data (list)\n",
    "\n",
    "def data_read(fileName):\n",
    "    with open (fileName, encoding = 'utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        first_row = next(reader, None)\n",
    "        data = list(reader)\n",
    "        return data, first_row\n",
    "\n",
    "def header_extension(data_list, startColumn, extension, new_header):\n",
    "    for i in range(startColumn, len(data_list)):\n",
    "        new_header.append(f'{data_list[i]} - {extension}')\n",
    "    return new_header\n",
    "    \n",
    "    \n",
    "# Subtract each features for each data in the same row (one-to-one) and save the data in total_list. List contains\n",
    "# the extracted feature values related to texts. Ex. total_list[0] = all feature values of first statement\n",
    "# To use absolute value, uncomment first feature and comment second feature.\n",
    "\n",
    "def one_to_one_dif(text, ref, startFColumn, feature_num):\n",
    "    feature_list = []\n",
    "    for i in range (startFColumn, feature_num):\n",
    "        try:\n",
    "            #feature = (abs(float(data2[i]) - float(data1[i])))\n",
    "            feature = (float(text[i]) - float(ref[i]))\n",
    "        except:\n",
    "            feature = (float(text[i]))\n",
    "                \n",
    "        feature_list.append(round(feature, 2))\n",
    "    return feature_list\n",
    "    \n",
    "    \n",
    "def one_to_many_dif(ref, text, feature_num, feature_pos):\n",
    "    avg_list = [];\n",
    "    for i in range(len(ref)):\n",
    "        feature_val = float(text[feature_pos]) - float(ref[i][feature_pos])\n",
    "        avg_list.append(feature_val)\n",
    "    avg = round(mean(avg_list), 2)\n",
    "    return avg\n",
    "    \n",
    "    \n",
    "# Input: news1 (String), news2 (String), outFile (String), startFColumn (integer)\n",
    "# Description: Read the data from news1 (reliable) and news2 (unreliable), and put the data in two lists. Then, subtract the news1 values in each column from news2 values.\n",
    "#              The subtraction starts from startFColumn until the end of the .csv file. Next, the results will be saved in the file with given name or path from user (outFile)\n",
    "#\n",
    "# Return: New DataFrame ('MS_features')\n",
    "\n",
    "def multi_source_FE(news1, news2, outFile, startFColumn):\n",
    "    \n",
    "    # Reading data from .csv files\n",
    "    data1 = data_read(f'./{news1}')\n",
    "    data2 = data_read(f'./{news2}')\n",
    "    \n",
    "    # Saving original headers in header\n",
    "    header = data1[1]\n",
    "    feature_num = len(header)\n",
    "    \n",
    "    # Create header list with modified header names. Can be changed later with different method of multi-sourcing by modifying '- sub'.\n",
    "    header_edit = ['text', 'news']\n",
    "    header_edit = header_extension(header, 2, 'sub', header_edit)\n",
    "    header_edit = header_extension(header, 2, 'avg sub', header_edit)\n",
    "    \n",
    "    #for i in range(startFColumn, len(data1[1])):\n",
    "     #   header.append(f'{data1[1][i]} - sub')\n",
    "      #  header.append(f'{data1[1][i]} - avg sub')\n",
    "    \n",
    "    # Assign data of the testing text and referencing text in data1 and data2\n",
    "    data1 = data1[0]\n",
    "    data2 = data2[0]\n",
    "    \n",
    "    # Create local lists, dataframe that are used later in this definition.\n",
    "    text = []\n",
    "    ref = []\n",
    "    total_list = []\n",
    "    new_total_list = []\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Loop through the data for subtraction\n",
    "    for i in range(len(data2)):\n",
    "        \n",
    "        # Statement of each data (text - text trying to verify, ref - reliable articles to reference for text comparison)\n",
    "        try:\n",
    "            ref.append(data1[i][1])\n",
    "            text.append(data2[i][1])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #print(one_to_one_dif(data1[i], data2[i], startFColumn, feature_num))\n",
    "        try:\n",
    "            total_list.append(one_to_one_dif(data1[i], data2[i], startFColumn, feature_num))\n",
    "            for j in range(startFColumn, feature_num):\n",
    "                total_list[i].append(one_to_many_dif(data1, data2[i], feature_num, j))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    # Reshape the lists in terms of features. Ex. new_total_list[0] = all values of first feature\n",
    "    for i in range(len(total_list)):\n",
    "        new_list = []\n",
    "        for element in (total_list):\n",
    "            try:\n",
    "                new_list.append(element[i])\n",
    "            except:\n",
    "                pass\n",
    "        new_total_list.append(new_list)\n",
    "       \n",
    "    # Creating dataframe with multi-sourced features\n",
    "    df['TEXT'] = text\n",
    "    df['REFERENCE'] = ref\n",
    "    for i in range(2, len(header_edit)):\n",
    "        df[header_edit[i]] = new_total_list[i-2]\n",
    "    \n",
    "    # Saving data into file\n",
    "    df.to_csv(f'./{outFile}')\n",
    "    return df\n",
    "\n",
    "# Initiate the code and check for the dataframe\n",
    "MSFE = multi_source_FE('news_FE.csv', 'text_FE.csv', 'OtoM_FE.csv', 2)\n",
    "print(MSFE.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
