{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\\n\n",
    "# For NewsAPI\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# For Individual Feature Extraction \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# For Relational Feature Extraction\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "# For feature analyzation\n",
    "#enable multiple outputs per cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# increase size of output window\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# For machine learning\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Modeling\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Cross-validation\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Saving\n",
    "import joblib\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n"
     ]
    }
   ],
   "source": [
    "header = [\"ID\", \"label\", \"description\", 'subject', 'speaker', 'speaker_job', 'state_info', 'party_aff', 'barely_true', 'false', 'half_true', 'mostly_true', 'pants_on_fire', 'context']\n",
    "data = pd.read_csv(\"./datasets/liar_dataset/train.tsv\", delimiter = '\\t', names = header, encoding = 'unicode_escape')\n",
    "\n",
    "#Statement = data['statement']\n",
    "#Label = data['label']\n",
    "subject_list = []\n",
    "temp = []\n",
    "subject = data['subject']\n",
    "\n",
    "#print(subject)\n",
    "\n",
    "\n",
    "subject = subject.tolist()\n",
    "for element in subject:\n",
    "    try:\n",
    "        #print(element.split(','))\n",
    "        for i in element.split(','):\n",
    "            temp.append(i)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "subject_list = set(temp)\n",
    "print(len(subject_list))\n",
    "\n",
    "        \n",
    "#print(list_two)\n",
    "        \n",
    "#subjects = set(subject)\n",
    "#print(len(subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_api(topic, number, source):\n",
    "    # List creation\n",
    "    source_id = []\n",
    "    author = []\n",
    "    title = []\n",
    "    description = []\n",
    "    url = []\n",
    "    content = []\n",
    "    pub_date = []\n",
    "    rel = []\n",
    "\n",
    "    # Read 100 news articles about coronavirus (20 articles per each page, 5 pages) and parse each data into corresponding lists\n",
    "    for i in range(1, int((number/20)+1)):\n",
    "\n",
    "        # News extraction\n",
    "        news_url = ('https://newsapi.org/v2/everything?'\n",
    "                f'domains={source}&'\n",
    "                f'q={topic}&'\n",
    "               f'page={i}&'\n",
    "               'sortBy=popularity&'\n",
    "               'apiKey=ddb22f3ef2b54abe8af228db83421424')\n",
    "\n",
    "        news = requests.get(news_url)\n",
    "        \n",
    "        #print((news.json()['totalResults']))\n",
    "        print(news.json())\n",
    "        \n",
    "        \n",
    "        for elements in news.json()['articles']:\n",
    "            source_id.append(elements['source']['id'])\n",
    "            author.append(elements['author'])\n",
    "            title.append(elements['title'])\n",
    "            description.append(elements['description'])\n",
    "            url.append(elements['url'])\n",
    "            content.append(elements['content'])\n",
    "            pub_date.append(elements['publishedAt'])\n",
    "\n",
    "            # *For CNN Only: check if the content is video and data does not related to the article\n",
    "            #if (elements['content'] == 'None' or elements['content'] == \\\"Chat with us in Facebook Messenger. Find out what's happening in the world as it unfolds.\\\"):\n",
    "            #    pass\n",
    "                #print(elements['description'])\n",
    "            #else:\n",
    "                #print (elements['content'])\n",
    "            #    pass\n",
    "    \n",
    "    # Test for parsed data\n",
    "    #print(author)\n",
    "    news_info = pd.DataFrame()\n",
    "    news_info['source_id'] = source_id\n",
    "    news_info['author'] = author\n",
    "    news_info['title'] = title\n",
    "    news_info['description'] = description\n",
    "    news_info['url'] = url\n",
    "    news_info['content'] = content\n",
    "    news_info['published_date'] = pub_date\n",
    "    return news_info\n",
    "    \n",
    "    \n",
    "#news = news_api('economy', 100, 'www.cbc.ca', '2020-01-30', 1)\n",
    "#text = news_api('economy', 100, 'youtube.com, wsj.com', '2020-01-30', 0)\n",
    "\n",
    "#ref = news_api('economy', 100, 'cnn.com', '2020-01-30', 1)\n",
    "\n",
    "#print(news['description'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the lists of the words that used for feature extraction\n",
    "\n",
    "positive_list = list()\n",
    "negative_list = list()\n",
    "\n",
    "# Gather the positive and negative word lists from the text file\n",
    "with open('./word_sentiment/positive-words.txt') as p:\n",
    "    for line in p:\n",
    "        val = line.split()\n",
    "        for ele in val:\n",
    "            positive_list.append(ele)\n",
    "\n",
    "with open('./word_sentiment/negative-words.txt') as n:\n",
    "    for line in n:\n",
    "        val = line.split()\n",
    "        for ele in val:\n",
    "            negative_list.append(ele)\n",
    "\n",
    "subjective_list = list(['am', 'are', 'is', 'was', 'were', 'be', 'been'])\n",
    "causation_list = list(['led to', 'because', 'cause', 'caused', 'reason', 'explanation', 'so'])\n",
    "\n",
    "exclusive_list = list(['except', 'else', 'besides', 'without', 'exclude', 'other than'])\n",
    "generalizing_list = list(['all', 'none', 'most', 'many', 'always', 'everyone','never',\n",
    "                          'some','usually','few','seldom','generally','general','overall'])\n",
    "pronoun_1st_list = list(['I','we'])\n",
    "pronoun_2nd3rd_list = list(['you','your','yours','he','she','it','him','her','his','her','its','hers','They','them','theirs','their'])\n",
    "\n",
    "SPronoun_list = list(['I', 'mine','my','me'])\n",
    "GPronoun_list = list(['we','ours','our','us'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the dataset, isolate the required data (statement and label in this program) and return as pandas Series\n",
    "# If the user needs the required data as a file, uncomment def with new_file as argument and comment the other one\n",
    "\n",
    "#def setup_data(file_name, new_file):\n",
    "def setup_data(news_info):\n",
    "    \n",
    "    # Check for data reading\n",
    "    #print(news_info.head(5))\n",
    "    \n",
    "    # Check for the raw data from original dataset\n",
    "    # print (list(data.columns.values))\n",
    "    # print(data.tail(1))\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "    news_contents = []\n",
    "    label_list = []\n",
    "    subject_list = []\n",
    "    \n",
    "    for elements in range(len(news_info)):\n",
    "        description = news_info.loc[elements, 'description']\n",
    "        label = news_info.loc[elements, 'label']\n",
    "        subject = news_info.loc[elements, 'subject']\n",
    "        \n",
    "        label = label.replace(\"half-true\", \"1\")\n",
    "        label = label.replace(\"mostly-true\", \"1\")\n",
    "        label = label.replace(\"barely-true\",\"0\")\n",
    "        label = label.replace(\"TRUE\",\"1\")\n",
    "        label = label.replace(\"pants-fire\",\"0\")\n",
    "        label = label.replace(\"FALSE\",\"0\")\n",
    "\n",
    "        \n",
    "        # uncomment and indent the news_contents.append(description) line if both contents and descriptions are used instead of  description only\n",
    "        \n",
    "        #content = data.loc[elements, 'content']\n",
    "        #if not pd.isna(content) and content is \\\"Chat with us in Facebook Messenger. Find out what's happening in the world as it unfolds.\\\":\n",
    "        #    news_contents.append(content)\n",
    "        #else:\n",
    "        news_contents.append(description)\n",
    "        label_list.append(label)\n",
    "        subject_list.append(subject)\n",
    "    \n",
    "    data['description'] = news_contents\n",
    "    data['label'] = label_list\n",
    "    data['subject'] = subject_list\n",
    "    \n",
    "    #try:\n",
    "    for i in range(len(data)):\n",
    "        data['label'][i] = int(data['label'][i])\n",
    "    #except:\n",
    "    #    print('something passed')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return data\n",
    "\n",
    "# Data Extraction\n",
    "# From the dataset file, gather the news contents and labels only\n",
    "\n",
    "#text_data = setup_data(text)\n",
    "#news_data = setup_data(news)\n",
    "#ref_data = setup_data(ref)\n",
    "\n",
    "#print(text_data.head(5))\n",
    "#print(news_data.head(5))\n",
    "\n",
    "text = setup_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: String ('str')\n",
    "# Description: Tockenize and tag input with nltk universal tag\n",
    "# Return: tags of each tockens - String ('tagged')\n",
    "# Tockenize and tag input String with nltk universal tag and return the tags for each words in the String\n",
    "\n",
    "def tagging_univ(str):\n",
    "    text = nltk.word_tokenize(str)\n",
    "    tagged = nltk.pos_tag(text, tagset='universal')\n",
    "    return tagged\n",
    "\n",
    "# Input: String('str')\n",
    "# Description: Tockenize and tag input with nltk non-universal tag\n",
    "# Return: tags of each tockens - String ('tagged')\n",
    "\n",
    "def tagging_nuniv(str):\n",
    "    text = nltk.word_tokenize(str)\n",
    "    tagged = nltk.pos_tag(text)\n",
    "    return tagged\n",
    "\n",
    "# Input: String('str')\n",
    "# Description: Count the number of characters in input\n",
    "# Return: Character count - int ('count')\n",
    "\n",
    "def count_char(str):\n",
    "    no_space = str.replace(\" \", \"\")\n",
    "    count = len(no_space)\n",
    "    return count\n",
    "\n",
    "# Input: String('str')\n",
    "# Description: Count the number of words in input\n",
    "# Return: Word count - int ('count')\n",
    "\n",
    "def count_word(str):\n",
    "    count = len(str.split())\n",
    "    return count\n",
    "\n",
    "# Input: String ('str')\n",
    "# Description: Count the number of sentences by counting number of period(.)\n",
    "# Return: Sentence count - int ('sentence')\n",
    "\n",
    "def count_sent(str):\n",
    "    sentence = len(str.split('.'))\n",
    "    return sentence\n",
    "\n",
    "# Input: String ('states')\n",
    "# Description: Count the number of characters in each word in input and average the number of characters per word\n",
    "# Return: Average number of characters: float ('avg')\n",
    "\n",
    "def count_char_per_word(states):\n",
    "    word = []\n",
    "    word.append(states.split())\n",
    "    char_per_word = list()\n",
    "    for elements in word:\n",
    "        for char in elements:\n",
    "            c_in_w_count = len(char)\n",
    "            char_per_word.append(c_in_w_count)\n",
    "    # char_per_word_list.append()\n",
    "    avg = sum(char_per_word) / len(char_per_word)\n",
    "    char_per_word.clear()\n",
    "    return avg\n",
    "\n",
    "# Input: String ('tagged'), String ('tag'), count ('int')\n",
    "# Description: Count the number of specified tags ('tag') from the input\n",
    "# Return: Count of specified tag - int ('int')\n",
    "\n",
    "def check_tag(check, tag, count):\n",
    "    if check == tag:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "# Input: String ('tagged'), list ('list'), int ('int')\n",
    "# Description: Count the number of common words between input String and input list\n",
    "# Return: Count of common words - int ('int')\n",
    "# Count and return the number of words that is included in the given list in the statement\n",
    "\n",
    "def check_common(state, list, count):\n",
    "    #for ele in list:\n",
    "    #    count = tagged.count(ele)\n",
    "    #return count\n",
    "    for elements in list:\n",
    "        for words in state.split():\n",
    "            if words == elements:\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "# Input: String ('word'), String ('tag'), list ('subjective_list'), int ('sent_count')\n",
    "# Description: Count the number of passive voice and avarge it from number of sentence (sent_count)\n",
    "# Return: Average of passive voiced sentences - float ('result')\n",
    "\n",
    "def count_passive(word, tag, subjective_list, sent_count):\n",
    "    percent_sub = 0\n",
    "    counter = 0\n",
    "    for ele in subjective_list:\n",
    "        if (word.count(ele) > 0 and tag == \"VBN\"):\n",
    "            counter += 1\n",
    "    result = counter / sent_count * 100\n",
    "    return result\n",
    "\n",
    "# Input: String ('states')\n",
    "# Description: Count the words that introduced only once in input\n",
    "# Return: Count of unique words - int ('unique_count')\n",
    "\n",
    "def count_unique(states):\n",
    "    words = states.split(' ')\n",
    "    c = Counter(words)\n",
    "    unique = [w for w in words if c[w] == 1]\n",
    "    unique_counter = len(unique)\n",
    "    return unique_counter\n",
    "\n",
    "# Calvin\n",
    "def avg_char(states):\n",
    "    word.append(states.split())\n",
    "    char_per_word = list()\n",
    "    for elements in word:\n",
    "        for char in elements:\n",
    "            c_in_w_count = len(char)\n",
    "            char_per_word.append(c_in_w_count)\n",
    "    word.clear()\n",
    "    avg = sum(char_per_word)/len(char_per_word)\n",
    "    char_per_word.clear()\n",
    "    return avg\n",
    "\n",
    "\n",
    "# Input: panda Series ('data'), String ('save_file_name')\n",
    "# Description: First, this function creates lists for each features and extract the features using statements in dataset and\n",
    "#              above functions. Next, it creates a large pandas Series that consist of news contents, labels and extracted\n",
    "#              features. Finally, it save the final pandas Series in a file to make easier to examine the result (do not need\n",
    "#              to rerun the program or change the code to check raw data)\n",
    "# Return: Pandas Series consist of news contents, labels and extracted features count - pandas Series ('new')\n",
    "\n",
    "def feature_extract(data):\n",
    "    # define the news contents and labels from the dataset\n",
    "    state = data['description']\n",
    "    #label = data['label']\n",
    "    #subject = data['subject']\n",
    "    \n",
    "    # Create a dataframe that will store every feature values\n",
    "    ind_feature = pd.DataFrame()\n",
    "\n",
    "    # create lists for storing the counters\n",
    "    char_count_list = list()\n",
    "    word_count_list = list()\n",
    "    verb_count_list = list()\n",
    "    noun_count_list = list()\n",
    "    sent_count_list = list()\n",
    "    words_per_sent_list = list()\n",
    "    char_per_word_list = list()\n",
    "    quest_count_list = list()\n",
    "    sub_count_list = list()\n",
    "    pass_count_list = list()\n",
    "    pos_count_list = list()\n",
    "    neg_count_list = list()\n",
    "    unique_count_list = list()\n",
    "    typo_count_list = list()\n",
    "    cause_count_list = list()\n",
    "    \n",
    "    # Calvin\n",
    "    gene_count_list = list()\n",
    "    num_count_list = list()\n",
    "    pron_1st_count_list = list()\n",
    "    pron_2nd3rd_count_list = list()\n",
    "    exclu_count_list = list()\n",
    "    \n",
    "    # Andrew\n",
    "    exclam_list = list()\n",
    "    lex_list = list()\n",
    "    singlulars_list = list()\n",
    "    group_list = list()\n",
    "    other_list = list()\n",
    "\n",
    "\n",
    "    word = list()\n",
    "\n",
    "    #print(type(state))\n",
    "    # loop for checking each new contents in dataset\n",
    "    for states in state:\n",
    "\n",
    "        #print(states)\n",
    "        # reset the counters for each news contents\n",
    "        w_in_s_count = 0\n",
    "        c_in_w_count = 0\n",
    "        verb_count = 0\n",
    "        noun_count = 0\n",
    "        sub_count = 0\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        percent_pos = 0\n",
    "        percent_neg = 0\n",
    "        unique_count = 0\n",
    "        sent_counts = 0\n",
    "        typo_count = 0\n",
    "        cause_count = 0\n",
    "        gene_count = 0\n",
    "        pron_1st_count = 0\n",
    "        pron_count = 0\n",
    "        exclu_count = 0\n",
    "        SelfP = 0\n",
    "        count_prongroup = 0\n",
    "        group = 0\n",
    "        other = 0\n",
    "        num= 0\n",
    "        \n",
    "        # Tockenization and tagging with nltk universal and non-universal tag systems (tagged = universal, tagged_nu = non-universal)\n",
    "        tagged = tagging_univ(states)\n",
    "        tagged_nu = tagging_nuniv(states)\n",
    "\n",
    "        # Check the tags of each news contents\n",
    "        # print(tagged)\n",
    "        # print(tagged_nu)\n",
    "\n",
    "        # Extract the features and append the results in the list. Commented lines with print() functions are for testing\n",
    "\n",
    "        # 1. Number of Characters\n",
    "        char_count = count_char(states)\n",
    "        char_count_list.append(char_count)\n",
    "\n",
    "        # 2. Nubmer of Words\n",
    "        word_count = count_word(states)\n",
    "        word_count_list.append(word_count)\n",
    "\n",
    "        # 3. Number of Verbs\n",
    "        for tag in tagged:\n",
    "            verb_count = check_tag(tag[1], 'VERB', verb_count)\n",
    "        verb_count_list.append(verb_count)\n",
    "\n",
    "        # 4. Number of Nouns\n",
    "        for tag in tagged:\n",
    "            noun_count = check_tag(tag[1], 'NOUN', noun_count)\n",
    "        noun_count_list.append(noun_count)\n",
    "        # print(noun_count)\n",
    "\n",
    "        # 5. Number of Sentence\n",
    "        sent_count = count_sent(states)\n",
    "        sent_count_list.append(sent_count)\n",
    "        # print(sent_count)\n",
    "\n",
    "        # 6. Average number of words per sentence\n",
    "        sent = [len(l.split()) for l in re.split(r'[?!.]', states) if l.strip()]\n",
    "        w_in_s_count = (sum(sent) / len(sent))\n",
    "        words_per_sent_list.append(w_in_s_count)\n",
    "        # print(w_in_s_count)\n",
    "\n",
    "        # 7. Average number of characters per word\n",
    "        c_in_w_count = count_char_per_word(states)\n",
    "        char_per_word_list.append(c_in_w_count)\n",
    "        # print(c_in_w_count)\n",
    "\n",
    "        # 8. Number of question marks\n",
    "        quest_count = states.count(\"?\")\n",
    "        quest_count_list.append(quest_count)\n",
    "\n",
    "        # 9. Percentage of subjective verbs - am/are/is/etc\n",
    "        sub_count = check_common(states, subjective_list, sub_count)\n",
    "        if (verb_count > 0):\n",
    "            percent_sub = sub_count / verb_count * 100\n",
    "        sub_count_list.append(percent_sub)\n",
    "\n",
    "        # 10. Percentage of passive voice - am/are/is && past participate\n",
    "        for tag in tagged_nu:\n",
    "            passive_percent = count_passive(tag[0], tag[1], subjective_list, sent_count)\n",
    "        pass_count_list.append(passive_percent)\n",
    "\n",
    "        # 11. Percentage of positive words\n",
    "        for tag in tagged:\n",
    "            pos_count = check_common(tag[0], positive_list, pos_count)\n",
    "            percent_pos = pos_count / word_count * 100\n",
    "        pos_count_list.append(percent_pos)\n",
    "\n",
    "        # 12. Percentage of negative words\n",
    "        for tag in tagged:\n",
    "            neg_count = check_common(tag[0], negative_list, neg_count)\n",
    "            percent_neg = neg_count / word_count * 100\n",
    "        neg_count_list.append(percent_neg)\n",
    "\n",
    "        # 13. Lexical diversity: unique words or terms\n",
    "        unique_count = count_unique(states)\n",
    "        unique_count_list.append(unique_count)\n",
    "\n",
    "        # 14. Typographical error ratio: misspelled words\n",
    "        for tag in tagged:\n",
    "            typo_count = check_tag(tag[1], 'X', typo_count)\n",
    "        typo_count_list.append(typo_count)\n",
    "\n",
    "        # 15. Causation terms\n",
    "        cause_count = check_common(states, causation_list, cause_count)\n",
    "        cause_count_list.append(cause_count)\n",
    "\n",
    "        # 16. Percentage of generalizing terms\n",
    "        gene_count = check_common(states, generalizing_list, gene_count)\n",
    "        gene_count_list.append(gene_count)\n",
    "        \n",
    "        # 17. Percentage of numbers and quantifiers\n",
    "        for tag in tagged:\n",
    "            num = check_tag(tag[1], 'NUM', num)\n",
    "        num_count_list.append(num)\n",
    "\n",
    "        # 18. 1st person pronouns\n",
    "        pron_1st_count = check_common(states, pronoun_1st_list, pron_1st_count)\n",
    "        pron_1st_count_list.append(pron_1st_count)\n",
    "   \n",
    "        # 19. 2nd and 3rd person pronouns\n",
    "        pron_count = check_common(states, pronoun_2nd3rd_list, pron_count)\n",
    "        pron_2nd3rd_count_list.append(pron_count)\n",
    "\n",
    "        # 20. Exclusive terms\n",
    "        exclu_count = check_common(states, exclusive_list, exclu_count)\n",
    "        exclu_count_list.append(exclu_count)\n",
    "        \n",
    "        # 21. # of exclamation marks\n",
    "        exclam_count = states.count(\"!\")\n",
    "        exclam_list.append(exclam_count)\n",
    "        \n",
    "        # 22. lexical \n",
    "        unique_count = count_unique(states)\n",
    "        lex_list.append(unique_count)\n",
    "        \n",
    "        # 23. singlular pronouns (1st person)\n",
    "        SelfP = check_common(states, SPronoun_list, SelfP)\n",
    "        singlulars_list.append(SelfP)\n",
    "        \n",
    "        # 24. Group ref pronouns (1st person)\n",
    "        group = check_common(states, GPronoun_list, group)\n",
    "        group_list.append(group)\n",
    "        \n",
    "        # 25. 2nd 3rd pronouns\n",
    "        other = check_common(states, pronoun_2nd3rd_list, other)\n",
    "        other_list.append(other)\n",
    "        \n",
    "\n",
    "    # Put the data into a dataframe\n",
    "        \n",
    "    #ind_feature['Statement'] = state\n",
    "    #ind_feature['Label'] = label\n",
    "    #ind_feature['Subject'] = subject\n",
    "    ind_feature['# of Characters'] = char_count_list\n",
    "    ind_feature['# of Words'] = word_count_list\n",
    "    ind_feature['# of Verbs'] = verb_count_list\n",
    "    ind_feature['# of Noun'] = noun_count_list\n",
    "    ind_feature['# of Sentence'] = sent_count_list\n",
    "    ind_feature['Average # of Words per Sentence'] = words_per_sent_list\n",
    "    ind_feature['Average # of Characters per Words'] = char_per_word_list\n",
    "    ind_feature['# of Question Marks'] = quest_count_list\n",
    "    ind_feature['% of Subjective Verbs'] = sub_count_list\n",
    "    ind_feature['% of Passive Voice'] = pass_count_list\n",
    "    ind_feature['% of Positive Words'] = pos_count_list\n",
    "    ind_feature['% of Negative Words'] = neg_count_list\n",
    "    ind_feature['# of Unique Wrods/Terms'] = unique_count_list\n",
    "    ind_feature['# of Misspelled Words'] = typo_count_list\n",
    "    ind_feature['# of Causation Terms'] = cause_count_list\n",
    "    ind_feature['% of generalizing terms'] = gene_count_list\n",
    "    ind_feature['% of # and quantifiers'] = num_count_list\n",
    "    ind_feature['1st person pronouns'] = pron_1st_count_list\n",
    "    ind_feature['2nd and 3rd person pronouns'] = pron_2nd3rd_count_list\n",
    "    ind_feature['Exclusive term'] = exclu_count_list\n",
    "    ind_feature['# of exclamation marks'] = exclam_list\n",
    "    ind_feature['Lexical'] = lex_list\n",
    "    ind_feature['Singular pronouns(1st person)'] = singlulars_list\n",
    "    ind_feature['Group ref pronouns(1st person)'] = group_list\n",
    "    ind_feature['2nd 3rd pronouns'] = other_list\n",
    "\n",
    "    return ind_feature\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# Feature Extraction\n",
    "\n",
    "#text_data_FE = feature_extract(text)\n",
    "\n",
    "# Checking data\n",
    "\n",
    "#print(text_data_FE.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_null(data):\n",
    "    # drop the null values (if both content and description are null)\n",
    "    data = data.dropna(how = 'any')\n",
    "    for i in range(len(data)):\n",
    "        try:\n",
    "            if (len(data['description'][i]) == 0):\n",
    "                data = data.drop(i)\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10239\n",
      "{'status': 'error', 'code': 'rateLimited', 'message': 'You have made too many requests recently. Developer accounts are limited to 500 requests over a 24 hour period (250 requests available every 12 hours). Please upgrade to a paid plan if you need more requests.'}\n",
      "abortion\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'ref' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-fb11c26972e7>\u001b[0m in \u001b[0;36mmulti_source_FE\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0msubjects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubjects\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[0mref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnews_api\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubjects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m             \u001b[0mref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdrop_null\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-d7a10eb0d711>\u001b[0m in \u001b[0;36mnews_api\u001b[1;34m(topic, number, source)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0melements\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'articles'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0msource_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melements\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'source'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'articles'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-fb11c26972e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;31m# Initiate the code and check for the dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m \u001b[0mtext_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmulti_source_FE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;31m#print(text_feature.head(5))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-fb11c26972e7>\u001b[0m in \u001b[0;36mmulti_source_FE\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubjects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_ref_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'ref' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Subtract each feature values for each data in the same row (one-to-one). List contains\n",
    "# the extracted feature values related to texts. Ex. feature_list[0] = all feature values of first statement\n",
    "# In case of there is null in the reference data, use two next reference data\n",
    "\n",
    "def one_to_one_dif(text, ref):\n",
    "    feature_list = []\n",
    "    feature = 0\n",
    "    for i in range (len(text)):\n",
    "        try:\n",
    "            feature = (float(text[i]) - float(ref[i]))\n",
    "            print('no error')\n",
    "        except:\n",
    "            try:\n",
    "                feature = (float(text[i]) - float(ref[i+2]))\n",
    "                print('maybe here?')\n",
    "            except:\n",
    "                print('error here')\n",
    "                pass\n",
    "\n",
    "        feature_list.append(round(feature, 2))\n",
    "    return feature_list\n",
    "    \n",
    "# Subtract each feature values for each data in the same row and repeat it with different references.(one-to-many)\n",
    "# Then, average the difference. List contains the extracted feature values related to texts.\n",
    "# Ex. final_list[0] = all feature values of first statement\n",
    "# In case of there is null in the reference data, use two next reference data\n",
    "\n",
    "def one_to_many_dif(text, ref):\n",
    "    avg_list = []\n",
    "    final_list = []\n",
    "    for i in range(len(text)):\n",
    "        for j in range(len(ref)):\n",
    "            try:\n",
    "                feature_val = float(text[i]) - float(ref[j])\n",
    "            except:\n",
    "                try:\n",
    "                    feature_val = float(text[i]) - float(ref[i+2])\n",
    "                except:\n",
    "                    pass\n",
    "            avg_list.append(feature_val)\n",
    "        avg = round(mean(avg_list), 2)\n",
    "        final_list.append(avg)\n",
    "    return final_list\n",
    "    \n",
    "    \n",
    "# Input: news1 (String), news2 (String), outFile (String), startFColumn (integer)\n",
    "# Description: Read the data from news1 (reliable) and news2 (unreliable), and put the data in two lists. Then, subtract the news1 values in each column from news2 values.\n",
    "#              The subtraction starts from startFColumn until the end of the .csv file. Next, the results will be saved in the file with given name or path from user (outFile)\n",
    "#\n",
    "# Return: New DataFrame ('MS_features')\n",
    "\n",
    "def multi_source_FE(text):\n",
    "\n",
    "    # Saving original headers in header\n",
    "    #header = list(text)\n",
    "    \n",
    "    source = \"cnn.com\"\n",
    "    \n",
    "    # Only feature values\n",
    "    #cols = [col for col in text.columns if col not in ['Statement', 'Label', 'Subject']]\n",
    "    \n",
    "    state_text = text['description']\n",
    "    subject_text = text['subject']\n",
    "    label_text = text['label']\n",
    "    data_text = feature_extract(text)\n",
    "    #print(data_text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # = 42\n",
    "    #print(data_text['# of Characters'][0])\n",
    "    \n",
    "    #print(subject_text[0])\n",
    "    #ref = news_api('economy', 100, source)\n",
    "    #ref = drop_null(ref)\n",
    "    #ref_data = feature_extract(ref)\n",
    "    #ref_data\n",
    "    #print(ref_data)\n",
    "    \n",
    "    print(len(text))\n",
    "    \n",
    "    data_ref_list = []\n",
    "    \n",
    "    for subjects in subject_text:\n",
    "        try:\n",
    "            subjects = subjects.replace(',', ', ')\n",
    "            ref = news_api(subjects, 100, source)\n",
    "            ref = drop_null(ref)\n",
    "            data_ref = feature_extract(ref)\n",
    "            data_ref_list.append(data_ref)\n",
    "        except:\n",
    "            print(subjects)\n",
    "            print(ref)\n",
    "    print(data_ref_list)\n",
    "    \n",
    "    \n",
    "    #print(data_ref)\n",
    "    for feature_name in data_text:\n",
    "        \n",
    "        one_to_one = one_to_one_dif(data_text[feature_name], data_ref[feature_name])\n",
    "        #print(one_to_one)\n",
    "            #for i in range(len(data_text)):\n",
    "                #print(data_text[])\n",
    "\n",
    "        #print(data_text[k][text_data_row])\n",
    "        #print(k)\n",
    "            \n",
    "    \n",
    "#        for j in subject_text:\n",
    "        #print(i)\n",
    "#            pass\n",
    "        #news = news_api(subject_text, 100, source, '2020-02-02')\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    data_ref = ref[cols]\n",
    "\n",
    "    feature_num = len(data_text.keys())\n",
    "\n",
    "\n",
    "    # Create local lists, dataframe that are used later in this definition.\n",
    "    df = pd.DataFrame()\n",
    "    sub = pd.DataFrame()\n",
    "    avg_sub = pd.DataFrame()\n",
    "    text = []\n",
    "    ref = []\n",
    "    total_list = []\n",
    "    new_total_list = []\n",
    "    \n",
    "    # Loop through the data for subtraction\n",
    "    for i in data_text.keys():\n",
    "        sub[i + \\\"- sub\\\"] = one_to_one_dif(data_text[i], data_ref[i])\n",
    "        avg_sub[i + \\\"- avg sub\\\"] = one_to_many_dif(data_text[i], data_ref[i])\n",
    "    \n",
    "    # Creating dataframe with multi-sourced features\n",
    "    df['Statement'] = state_text\n",
    "    df['Label'] = label_text\n",
    "    \n",
    "    # Concatenate the text, reference and features, then drop null values\n",
    "    df1 = pd.concat([df, data_text, sub, avg_sub], axis = 1)\n",
    "    df1 = df1.dropna(how = 'any')\n",
    "    \n",
    "    return df1\n",
    "\"\"\"\n",
    "    \n",
    "# Initiate the code and check for the dataframe\n",
    "text_feature = multi_source_FE(text)\n",
    "\n",
    "#print(text_feature.head(5))\n",
    "#print(news_feature.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
