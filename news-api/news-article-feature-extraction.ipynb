{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Christophe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Christophe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Christophe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#enable multiple outputs per cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "\n",
    "newsapi = NewsApiClient(api_key=\"a1c9a16f08974282a0e45bed8577103b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting news articles as dataframe from a reliable news source\n",
    "def get_news_reliable(keyword):\n",
    "    articles = newsapi.get_everything(\n",
    "        q=keyword, \n",
    "        sources='cnn, cbs-news, bbc-news',\n",
    "        domains='http://www.cnn.com, http://www.cbsnews.com, http://www.bbc.com',\n",
    "        from_param='2020-01-01',\n",
    "        to='2020-01-28',\n",
    "        language='en',\n",
    "        sort_by='popularity',\n",
    "        page_size=100)\n",
    "    articles = pd.DataFrame(articles)\n",
    "    articles = pd.concat([articles.drop(['articles'], axis=1), articles['articles'].apply(pd.Series)], axis=1)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting news articles as dataframe from an unreliable news source\n",
    "def get_news_unreliable(keyword):\n",
    "    articles = newsapi.get_everything(\n",
    "        q=keyword, \n",
    "        sources='the-verge, fox-news',\n",
    "        domains='http://www.theverge.com, http://www.foxnews.com',\n",
    "        from_param='2020-01-01',\n",
    "        to='2020-01-28',\n",
    "        language='en',\n",
    "        sort_by='popularity',\n",
    "        page_size=100)\n",
    "    articles = pd.DataFrame(articles)\n",
    "    articles = pd.concat([articles.drop(['articles'], axis=1), articles['articles'].apply(pd.Series)], axis=1)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_dataFrame(articles):\n",
    "    article_results =[]\n",
    "    for i in range(len(file)):\n",
    "        article_dict={}\n",
    "        article_dict['title'] = file[i]['title']\n",
    "        article_dict['author'] = file[i]['author']\n",
    "        article_dict['source'] = file[i]['source']\n",
    "        article_dict['description'] = file[i]['description']\n",
    "        article_dict['content'] = file[i]['content']\n",
    "        article_dict['pub_date'] = file[i]['publishedAt']\n",
    "        article_dict['url'] = file[i]['url']\n",
    "        article_dict['photo_url'] = file[i]['urlToImage']\n",
    "        \n",
    "        article_results.append(article_dict)\n",
    "    return article_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_reliable = get_news_reliable('coronavirus')\n",
    "news_unreliable = get_news_unreliable('coronavirus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>totalResults</th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>urlToImage</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ok</td>\n",
       "      <td>293</td>\n",
       "      <td>{'id': 'bbc-news', 'name': 'BBC News'}</td>\n",
       "      <td>https://www.facebook.com/bbcnews</td>\n",
       "      <td>New China virus: North Korea bans foreign tour...</td>\n",
       "      <td>Nearly 300 people have been infected in the co...</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-asia-51194651</td>\n",
       "      <td>https://ichef.bbci.co.uk/news/1024/branded_new...</td>\n",
       "      <td>2020-01-21T19:33:22Z</td>\n",
       "      <td>Image copyrightGetty ImagesImage caption\\r\\n T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  status  totalResults                                  source  \\\n",
       "0     ok           293  {'id': 'bbc-news', 'name': 'BBC News'}   \n",
       "\n",
       "                             author  \\\n",
       "0  https://www.facebook.com/bbcnews   \n",
       "\n",
       "                                               title  \\\n",
       "0  New China virus: North Korea bans foreign tour...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Nearly 300 people have been infected in the co...   \n",
       "\n",
       "                                              url  \\\n",
       "0  https://www.bbc.co.uk/news/world-asia-51194651   \n",
       "\n",
       "                                          urlToImage           publishedAt  \\\n",
       "0  https://ichef.bbci.co.uk/news/1024/branded_new...  2020-01-21T19:33:22Z   \n",
       "\n",
       "                                             content  \n",
       "0  Image copyrightGetty ImagesImage caption\\r\\n T...  "
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>totalResults</th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>urlToImage</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ok</td>\n",
       "      <td>141</td>\n",
       "      <td>{'id': 'the-verge', 'name': 'The Verge'}</td>\n",
       "      <td>Nicole Wetsman</td>\n",
       "      <td>If youâ€™re feeling nervous about the coronaviru...</td>\n",
       "      <td>Reduce anxiety about the new coronavirus by pu...</td>\n",
       "      <td>https://www.theverge.com/2020/1/24/21080628/co...</td>\n",
       "      <td>https://cdn.vox-cdn.com/thumbor/aHRj-wE4dlipnM...</td>\n",
       "      <td>2020-01-24T21:50:50Z</td>\n",
       "      <td>Take a break from reading the internet\\r\\nPhot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  status  totalResults                                    source  \\\n",
       "0     ok           141  {'id': 'the-verge', 'name': 'The Verge'}   \n",
       "\n",
       "           author                                              title  \\\n",
       "0  Nicole Wetsman  If youâ€™re feeling nervous about the coronaviru...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Reduce anxiety about the new coronavirus by pu...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.theverge.com/2020/1/24/21080628/co...   \n",
       "\n",
       "                                          urlToImage           publishedAt  \\\n",
       "0  https://cdn.vox-cdn.com/thumbor/aHRj-wE4dlipnM...  2020-01-24T21:50:50Z   \n",
       "\n",
       "                                             content  \n",
       "0  Take a break from reading the internet\\r\\nPhot...  "
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_reliable.head(n=1)\n",
    "news_unreliable.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#extracting the descriptions of articles into array\n",
    "descrip_reliable = pd.Series(news_reliable.description.values)\n",
    "descrip_unreliable = pd.Series(news_unreliable.description.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove null entries\n",
    "descrip_reliable = descrip_reliable.dropna()\n",
    "descrip_unreliable = descrip_unreliable.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the lists of the words that used for feature extraction\n",
    "positive_list = list()\n",
    "negative_list = list()\n",
    "\n",
    "# Gather the positive and negative word lists from the text file\n",
    "with open('./word_sentiment/positive-words.txt', encoding='ANSI') as p:\n",
    "    for line in p:\n",
    "        val = line.split()\n",
    "        positive_list.append(val)\n",
    "\n",
    "with open('./word_sentiment/negative-words.txt', encoding='ANSI') as n:\n",
    "    for line in n:\n",
    "        val = line.split()\n",
    "        negative_list.append(val)\n",
    "\n",
    "subjective_list = list(['am ', 'are ', 'is ', 'was ', 'were ', 'be ', 'been '])\n",
    "causation_list = list(['led to ', 'because ', 'cause ', 'reason ', 'explanation ', 'so '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "\n",
    "# Not required for right now, but may used in later for more efficient tockenization and feature extraction\n",
    "# vectorizer = CountVectorizer()\n",
    "# train_count = vectorizer.fit_transform(data[\"statement\"].values)\n",
    "\n",
    "# Check for vectorizer\n",
    "# print(train_count.shape)\n",
    "# print(vectorizer.vocabulary_)\n",
    "# print(vectorizer.get_feature_names())\n",
    "\n",
    "# Input: String ('str')\n",
    "# Description: Tockenize and tag input with nltk universal tag\n",
    "# Return: tags of each tockens - String ('tagged')\n",
    "# Tockenize and tag input String with nltk universal tag and return the tags for each words in the String\n",
    "\n",
    "def tagging_univ(str):\n",
    "    text = nltk.word_tokenize(str)\n",
    "    tagged = nltk.pos_tag(text, tagset='universal')\n",
    "    return tagged\n",
    "\n",
    "# Input: String('str')\n",
    "# Description: Tockenize and tag input with nltk non-universal tag\n",
    "# Return: tags of each tockens - String ('tagged')\n",
    "\n",
    "def tagging_nuniv(str):\n",
    "    text = nltk.word_tokenize(str)\n",
    "    tagged = nltk.pos_tag(text)\n",
    "    return tagged\n",
    "\n",
    "# Input: String('str')\n",
    "# Description: Count the number of characters in input\n",
    "# Return: Character count - int ('count')\n",
    "\n",
    "def count_char(str):\n",
    "    no_space = str.replace(\" \", \"\")\n",
    "    count = len(no_space)\n",
    "    return count\n",
    "\n",
    "# Input: String('str')\n",
    "# Description: Count the number of words in input\n",
    "# Return: Word count - int ('count')\n",
    "\n",
    "def count_word(str):\n",
    "    count = len(str.split())\n",
    "    return count\n",
    "\n",
    "# Input: String ('tags'), int(verb_count)\n",
    "# Description: Count the number of verbs in input\n",
    "# Return: Verb count - int ('verb_count')\n",
    "\n",
    "def count_verb(tags, verb_count):\n",
    "    if (tags == 'VERB'):\n",
    "        verb_count += 1\n",
    "    return verb_count\n",
    "\n",
    "# Input: String('tags'), int('noun_count')\n",
    "# Description: Count the number of nouns in input\n",
    "# Return: Noun count - int ('noun_count')\n",
    "\n",
    "def count_noun(tags, noun_count):\n",
    "    if (tags == 'NOUN'):\n",
    "        noun_count += 1\n",
    "    return noun_count\n",
    "\n",
    "# Input: String ('str')\n",
    "# Description: Count the number of sentences by counting number of period(.)\n",
    "# Return: Sentence count - int ('sentence')\n",
    "\n",
    "def count_sent(str):\n",
    "    sentence = len(str.split('.'))\n",
    "    return sentence\n",
    "\n",
    "# Input: String ('states')\n",
    "# Description: Count the number of characters in each word in input and average the number of characters per word\n",
    "# Return: Average number of characters: float ('avg')\n",
    "\n",
    "def count_char_per_word(states):\n",
    "    word = []\n",
    "    word.append(states.split())\n",
    "    char_per_word = list()\n",
    "    for elements in word:\n",
    "        for char in elements:\n",
    "            c_in_w_count = len(char)\n",
    "            char_per_word.append(c_in_w_count)\n",
    "    # char_per_word_list.append()\n",
    "    avg = sum(char_per_word) / len(char_per_word)\n",
    "    char_per_word.clear()\n",
    "    return avg\n",
    "\n",
    "# Input: String ('tagged'), list ('list'), int ('int')\n",
    "# Description: Count the number of common words between input String and input list\n",
    "# Return: Count of common words - int ('int')\n",
    "# Count and return the number of common words between input String ('tagged') and input list\n",
    "\n",
    "def check_common(tagged, list, int):\n",
    "    for elements in list:\n",
    "        if tagged in elements:\n",
    "            int += 1\n",
    "            return int\n",
    "    return int\n",
    "\n",
    "# Input: list ('subjective_list'), int ('verb_count'), String ('states')\n",
    "# Description: Count the number of subjective words and avarage it from number of verbs in input\n",
    "# Return: Average of subjective words - float ('percent_sub')\n",
    "\n",
    "def count_sub(subjective_list, verb_count, states):\n",
    "    percent_sub = 0\n",
    "    for ele in subjective_list:\n",
    "        sub_count = states.count(ele)\n",
    "    if (verb_count > 0):\n",
    "        percent_sub = sub_count / verb_count * 100\n",
    "    return percent_sub\n",
    "\n",
    "# Input: String ('word'), String ('tag'), list ('subjective_list'), int ('sent_count')\n",
    "# Description: Count the number of passive voice and avarge it from number of sentence (sent_count)\n",
    "# Return: Average of passive voiced sentences - float ('result')\n",
    "\n",
    "def count_passive(word, tag, subjective_list, sent_count):\n",
    "    percent_sub = 0\n",
    "    counter = 0\n",
    "    for ele in subjective_list:\n",
    "        if (word.count(ele) > 0 and tag == \"VBN\"):\n",
    "            counter += 1\n",
    "    result = counter / sent_count * 100\n",
    "    return result\n",
    "\n",
    "# Input: String ('states')\n",
    "# Description: Count the words that introduced only once in input\n",
    "# Return: Count of unique words - int ('unique_count')\n",
    "\n",
    "def count_unique(states):\n",
    "    words = states.split(' ')\n",
    "    c = Counter(words)\n",
    "    unique = [w for w in words if c[w] == 1]\n",
    "    unique_counter = len(unique)\n",
    "    return unique_counter\n",
    "\n",
    "# Input: String ('tag'), int ('typo_count')\n",
    "# Description: Count the number of words with nltk universal tag is \"X\" (misspelled)\n",
    "# Return: Count of misspelled words - int ('typo_count')\n",
    "\n",
    "def count_typo(tag, typo_count):\n",
    "    if (tag == \"X\"):\n",
    "        typo_count += 1\n",
    "    return typo_count\n",
    "\n",
    "# Input: list ('causation_list'), String ('states')\n",
    "# Description: Count the number of common words in list input and String input\n",
    "# Return: Count of common words - int ('cause_count')\n",
    "\n",
    "def count_cause(causation_list, states):\n",
    "    for ele in causation_list:\n",
    "        cause_count = states.count(ele)\n",
    "    return cause_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: panda Series ('data'), String ('save_file_name')\n",
    "# Description: First, this function creates lists for each features and extract the features using statements in dataset and\n",
    "#              above functions. Next, it creates a large pandas Series that consist of news contents, labels and extracted\n",
    "#              features. Finally, it save the final pandas Series in a file to make easier to examine the result (do not need\n",
    "#              to rerun the program or change the code to check raw data)\n",
    "# Return: Pandas Series consist of news contents, labels and extracted features count - pandas Series ('new')\n",
    "\n",
    "def feature_extract(data, save_file_name):\n",
    "    # define the news contents and labels from the dataset\n",
    "    state = data\n",
    "\n",
    "    # create lists for storing the counters\n",
    "    char_count_list = list()\n",
    "    word_count_list = list()\n",
    "    verb_count_list = list()\n",
    "    noun_count_list = list()\n",
    "    sent_count_list = list()\n",
    "    words_per_sent_list = list()\n",
    "    char_per_word_list = list()\n",
    "    quest_count_list = list()\n",
    "    sub_count_list = list()\n",
    "    pass_count_list = list()\n",
    "    pos_count_list = list()\n",
    "    neg_count_list = list()\n",
    "    unique_count_list = list()\n",
    "    typo_count_list = list()\n",
    "    cause_count_list = list()\n",
    "    word = list()\n",
    "\n",
    "    #print(type(state))\n",
    "    # loop for checking each new contents in dataset\n",
    "    for states in state:\n",
    "\n",
    "        #print(states)\n",
    "        # reset the counters for each news contents\n",
    "        w_in_s_count = 0\n",
    "        c_in_w_count = 0\n",
    "        verb_count = 0\n",
    "        noun_count = 0\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        percent_pos = 0\n",
    "        percent_neg = 0\n",
    "        unique_count = 0\n",
    "        sent_counts = 0\n",
    "        typo_count = 0\n",
    "\n",
    "        # Tockenization and tagging with nltk universal and non-universal tag systems (tagged = universal, tagged_nu = non-universal)\n",
    "        tagged = tagging_univ(states)\n",
    "        tagged_nu = tagging_nuniv(states)\n",
    "\n",
    "        # Check the tags of each news contents\n",
    "        # print(tagged)\n",
    "        # print(tagged_nu)\n",
    "\n",
    "        # Extract the features and append the results in the list. Commented lines with print() functions are for testing\n",
    "\n",
    "        # 1. Number of Characters\n",
    "        char_count = count_char(states)\n",
    "        char_count_list.append(char_count)\n",
    "\n",
    "        # 2. Nubmer of Words\n",
    "        word_count = count_word(states)\n",
    "        word_count_list.append(word_count)\n",
    "\n",
    "        # 3. Number of Verbs\n",
    "        for tag in tagged:\n",
    "            verb_count = count_verb(tag[1], verb_count)\n",
    "        verb_count_list.append(verb_count)\n",
    "\n",
    "        # 4. Number of Nouns\n",
    "        for tag in tagged:\n",
    "            noun_count = count_noun(tag[1], noun_count)\n",
    "        noun_count_list.append(noun_count)\n",
    "        # print(noun_count)\n",
    "        \n",
    "        # 5. Number of Sentence\n",
    "        sent_count = count_sent(states)\n",
    "        sent_count_list.append(sent_count)\n",
    "\n",
    "        # 6. Average number of words per sentence\n",
    "        sent = [len(l.split()) for l in re.split(r'[?!.]', states) if l.strip()]\n",
    "        w_in_s_count = (sum(sent) / len(sent))\n",
    "        words_per_sent_list.append(w_in_s_count)\n",
    "\n",
    "        # 7. Average number of characters per word\n",
    "        c_in_w_count = count_char_per_word(states)\n",
    "        char_per_word_list.append(c_in_w_count)\n",
    "\n",
    "        # 8. Number of question marks\n",
    "        quest_count = states.count(\"?\")\n",
    "        quest_count_list.append(quest_count)\n",
    "\n",
    "        # 9. Percentage of subjective verbs - am/are/is/etc\n",
    "        percent_sub = count_sub(subjective_list, verb_count, states)\n",
    "        sub_count_list.append(percent_sub)\n",
    "\n",
    "        # 10. Percentage of passive voice - am/are/is && past participate\n",
    "        for tag in tagged_nu:\n",
    "            passive_percent = count_passive(tag[0], tag[1], subjective_list, sent_count)\n",
    "        pass_count_list.append(passive_percent)\n",
    "\n",
    "        # 11. Percentage of positive words\n",
    "        for tag in tagged:\n",
    "            pos_count = check_common(tag[0], positive_list, pos_count)\n",
    "            percent_pos = pos_count / word_count * 100\n",
    "        pos_count_list.append(percent_pos)\n",
    "\n",
    "        # 12. Percentage of negative words\n",
    "        for tag in tagged:\n",
    "            neg_count = check_common(tag[0], negative_list, neg_count)\n",
    "            percent_neg = neg_count / word_count * 100\n",
    "        neg_count_list.append(percent_neg)\n",
    "        \n",
    "        # 13. Lexical diversity: unique words or terms\n",
    "        unique_count = count_unique(states)\n",
    "        unique_count_list.append(unique_count)\n",
    "\n",
    "        # 14. Typographical error ratio: misspelled words\n",
    "        for tag in tagged:\n",
    "            typo_count = count_typo(tag[1], typo_count)\n",
    "        typo_count_list.append(typo_count)\n",
    "        \n",
    "        # 15. Causation terms\n",
    "        cause_count = count_cause(causation_list, states)\n",
    "        cause_count_list.append(cause_count)\n",
    "\n",
    "    # Convert lists into pandas Series\n",
    "    statement = pd.Series(state)\n",
    "    first = pd.Series(char_count_list)\n",
    "    second = pd.Series(word_count_list)\n",
    "    third = pd.Series(verb_count_list)\n",
    "    fourth = pd.Series(noun_count_list)\n",
    "    fifth = pd.Series(sent_count_list)\n",
    "    sixth = pd.Series(words_per_sent_list)\n",
    "    seventh = pd.Series(char_per_word_list)\n",
    "    eighth = pd.Series(quest_count_list)\n",
    "    nineth = pd.Series(sub_count_list)\n",
    "    tenth = pd.Series(pass_count_list)\n",
    "    eleventh = pd.Series(pos_count_list)\n",
    "    twelveth = pd.Series(neg_count_list)\n",
    "    thirteenth = pd.Series(unique_count_list)\n",
    "    fourteenth = pd.Series(typo_count_list)\n",
    "    fifteenth = pd.Series(cause_count_list)\n",
    "\n",
    "    # Concatenate all feature Series, news contents and labels into one pandas Series\n",
    "    new = pd.concat([statement, first, second, third, fourth, fifth, sixth, seventh, eighth, nineth, tenth,\n",
    "                     eleventh, twelveth, thirteenth, fourteenth, fifteenth], axis=1)\n",
    "\n",
    "    # Test for the final Series\n",
    "    # print(new.head(3))\n",
    "\n",
    "    # Name the header of the data\n",
    "    new_header = ['Statement', '# of Characters', '# of Words', '# of Verbs', '# of Noun', '# of Sentence',\n",
    "                  'Average # of Words per Sentence', 'Average # of Characters per Words', '# of Question Marks',\n",
    "                  '% of Subjective Verbs', '% of Passive Voice', '% of Positive Words', '% of Negative Words',\n",
    "                  '# of Unique Wrods/Terms', '# of Misspelled Words', '# of Causation Terms']\n",
    "\n",
    "    # Save the data in a file\n",
    "\n",
    "    #with open(save_file_name, 'w') as f:\n",
    "    #    writer = csv.DictWriter(f, fieldnames=new_header)\n",
    "    #    writer.writeheader()\n",
    "\n",
    "    new.to_csv(save_file_name, header=new_header)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Nearly 300 people have been infected in the co...</td>\n",
       "      <td>85.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A BBC team travels into Hubei province, where ...</td>\n",
       "      <td>72.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.538462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.692308</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Australian scientists say it is a \"significant...</td>\n",
       "      <td>89.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.933333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>The new coronavirus is said to be infectious d...</td>\n",
       "      <td>87.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.117647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Jose Ramirez's unified WBC and WBO light-welte...</td>\n",
       "      <td>127.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.047619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>The deadly coronavirus has sickened hundreds o...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>Social media network has blocked only some use...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>The coronavirus outbreak started in central Ch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>CBS News correspondent Ramy Inocencio has the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>Companies are offering workers up to five time...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0      1     2    3     4  \\\n",
       "0   Nearly 300 people have been infected in the co...   85.0  15.0  4.0   5.0   \n",
       "1   A BBC team travels into Hubei province, where ...   72.0  13.0  1.0   6.0   \n",
       "2   Australian scientists say it is a \"significant...   89.0  15.0  3.0   4.0   \n",
       "3   The new coronavirus is said to be infectious d...   87.0  17.0  6.0   3.0   \n",
       "4   Jose Ramirez's unified WBC and WBO light-welte...  127.0  21.0  2.0  11.0   \n",
       "..                                                ...    ...   ...  ...   ...   \n",
       "95  The deadly coronavirus has sickened hundreds o...   90.0  16.0  1.0   8.0   \n",
       "96  Social media network has blocked only some use...  100.0  20.0  3.0   7.0   \n",
       "97  The coronavirus outbreak started in central Ch...    NaN   NaN  NaN   NaN   \n",
       "98  CBS News correspondent Ramy Inocencio has the ...    NaN   NaN  NaN   NaN   \n",
       "99  Companies are offering workers up to five time...    NaN   NaN  NaN   NaN   \n",
       "\n",
       "      5     6         7    8     9   10         11         12    13   14   15  \n",
       "0   2.0  15.0  5.666667  0.0  25.0  0.0   0.000000  13.333333  13.0  0.0  0.0  \n",
       "1   2.0  13.0  5.538462  0.0   0.0  0.0   0.000000   7.692308  13.0  0.0  0.0  \n",
       "2   2.0  15.0  5.933333  0.0   0.0  0.0  13.333333   0.000000  15.0  0.0  0.0  \n",
       "3   2.0  17.0  5.117647  0.0   0.0  0.0   0.000000   0.000000  14.0  0.0  0.0  \n",
       "4   2.0  21.0  6.047619  0.0   0.0  0.0   0.000000   4.761905  21.0  0.0  0.0  \n",
       "..  ...   ...       ...  ...   ...  ...        ...        ...   ...  ...  ...  \n",
       "95  2.0  16.0  5.625000  0.0   0.0  0.0   0.000000   6.250000  14.0  0.0  0.0  \n",
       "96  2.0  20.0  5.000000  0.0   0.0  0.0   0.000000   5.000000  16.0  0.0  0.0  \n",
       "97  NaN   NaN       NaN  NaN   NaN  NaN        NaN        NaN   NaN  NaN  NaN  \n",
       "98  NaN   NaN       NaN  NaN   NaN  NaN        NaN        NaN   NaN  NaN  NaN  \n",
       "99  NaN   NaN       NaN  NaN   NaN  NaN        NaN        NaN   NaN  NaN  NaN  \n",
       "\n",
       "[100 rows x 16 columns]"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Reduce anxiety about the new coronavirus by pu...</td>\n",
       "      <td>124</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Google is trying to foreground sourcing and UR...</td>\n",
       "      <td>214</td>\n",
       "      <td>47</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>15.666667</td>\n",
       "      <td>4.553191</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.255319</td>\n",
       "      <td>2.127660</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Mysterious viral pneumonia outbreak in China l...</td>\n",
       "      <td>217</td>\n",
       "      <td>44</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>4.931818</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Developers of the popular game, in which playe...</td>\n",
       "      <td>134</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.620690</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.448276</td>\n",
       "      <td>6.896552</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The World Health Organization commended the de...</td>\n",
       "      <td>195</td>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.128205</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>Screenings for passengers arriving from Wuhan,...</td>\n",
       "      <td>102</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>5.368421</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>The Centers for Disease Control and Prevention...</td>\n",
       "      <td>223</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>5.868421</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>The man developed a cough while still in China...</td>\n",
       "      <td>97</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>4.850000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>The U.S. government is sending a charter fligh...</td>\n",
       "      <td>216</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>15.333333</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>We will likely have more cases (hopefully unco...</td>\n",
       "      <td>195</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>5.571429</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.714286</td>\n",
       "      <td>5.714286</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0    1   2   3   4  5  \\\n",
       "0   Reduce anxiety about the new coronavirus by pu...  124  24   3  10  2   \n",
       "1   Google is trying to foreground sourcing and UR...  214  47  12  15  3   \n",
       "2   Mysterious viral pneumonia outbreak in China l...  217  44   6  21  1   \n",
       "3   Developers of the popular game, in which playe...  134  29   5  11  2   \n",
       "4   The World Health Organization commended the de...  195  39   7  13  3   \n",
       "..                                                ...  ...  ..  ..  .. ..   \n",
       "95  Screenings for passengers arriving from Wuhan,...  102  19   3  11  2   \n",
       "96  The Centers for Disease Control and Prevention...  223  38   4  22  3   \n",
       "97  The man developed a cough while still in China...   97  20   3   5  2   \n",
       "98  The U.S. government is sending a charter fligh...  216  45   7  15  3   \n",
       "99  We will likely have more cases (hopefully unco...  195  35  11   7  2   \n",
       "\n",
       "            6         7  8    9   10        11         12  13  14  15  \n",
       "0   24.000000  5.166667  0  0.0  0.0  0.000000  12.500000  21   0   0  \n",
       "1   15.666667  4.553191  0  0.0  0.0  4.255319   2.127660  35   0   0  \n",
       "2   22.000000  4.931818  1  0.0  0.0  0.000000  11.363636  28   0   0  \n",
       "3   15.000000  4.620690  0  0.0  0.0  3.448276   6.896552  22   0   0  \n",
       "4   19.500000  5.000000  0  0.0  0.0  0.000000   5.128205  24   0   0  \n",
       "..        ...       ... ..  ...  ...       ...        ...  ..  ..  ..  \n",
       "95  19.000000  5.368421  0  0.0  0.0  0.000000   0.000000  19   0   0  \n",
       "96  13.000000  5.868421  0  0.0  0.0  0.000000   0.000000  30   0   0  \n",
       "97  20.000000  4.850000  0  0.0  0.0  5.000000   5.000000  16   0   0  \n",
       "98  15.333333  4.800000  0  0.0  0.0  0.000000   6.666667  33   0   0  \n",
       "99  35.000000  5.571429  0  0.0  0.0  5.714286   5.714286  25   0   0  \n",
       "\n",
       "[100 rows x 16 columns]"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "# Feature Extraction\n",
    "feature_extract(descrip_reliable, './reliable_FE.csv')\n",
    "feature_extract(descrip_unreliable, './unreliable_FE.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
